[
  {
    "objectID": "slides/1-intro/lecture4.html#machine-learning-engineering",
    "href": "slides/1-intro/lecture4.html#machine-learning-engineering",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Machine learning engineering",
    "text": "Machine learning engineering\n\n\n\n\n\n\n\n\n\nStudy the problem üîç\n\nExploratory data analysis\nDomain knowledge\nData preparation\n\n\n\n\n\nTrain ML model üíª\n\nAlgorithm/software development\n\n\n\n\n\n\nEvaluate üìà\n\nMetric selection\nModel selection\nUncertainty quantification\n\n\n\n\n\nLaunch üöÄ / Analyse errors ‚ùå\n\nDeployment\nDiagnostics and visualisation"
  },
  {
    "objectID": "slides/1-intro/lecture4.html#model-training",
    "href": "slides/1-intro/lecture4.html#model-training",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Model training",
    "text": "Model training\nDetails of different ML models is the topic for Module 2\nFor now, let‚Äôs represent a generic ML model as a function \\(\\color{Purple}{f}\\):\n\n\\[\n\\large \\hat{\\color{DarkBlue}{y}} = \\color{Purple}{f}(\\color{DarkOrange}{\\mathbf{x}}, \\boldsymbol{\\color{teal}{\\theta}})\n\\]\n\nwhere\n\n\\(\\hat{\\color{DarkBlue}{y}}\\) is the prediction\n\\(\\color{DarkOrange}{\\mathbf{x}}\\)¬†is a data point\n\\(\\boldsymbol{\\color{teal}{\\theta}}\\) are parameters of the model\n\n\nTraining is the process of finding the best parameters \\(\\boldsymbol{\\color{teal}{\\theta}}\\)¬†so that the prediction \\(\\hat{\\color{DarkBlue}{y}}\\) is as close as possible to the known target value \\(\\color{DarkBlue}{y}\\)."
  },
  {
    "objectID": "slides/1-intro/lecture4.html#model-training-1",
    "href": "slides/1-intro/lecture4.html#model-training-1",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Model training",
    "text": "Model training\nDetails of different ML models is the topic for Module 2\nFor now, let‚Äôs represent a generic ML model as a function \\(\\color{Purple}{f}\\):\n\n\\[\n\\large \\hat{\\color{DarkBlue}{y}} = \\color{Purple}{f}(\\color{DarkOrange}{\\mathbf{x}}, \\boldsymbol{\\color{teal}{\\theta}})\n\\]\n\n\nTraining is the process of finding the best parameters \\(\\boldsymbol{\\color{teal}{\\theta}}\\)¬†so that the prediction \\(\\hat{\\color{DarkBlue}{y}}\\) is as close as possible to the known target value \\(\\color{DarkBlue}{y}\\).\n\nscikit-learn API:\n\n\n# Generic model API\nfrom sklearn.model_family import ModelName\n\nmodel = ModelName(some_setting=\"foo\")\nmodel.fit(X, y) # X are data, y are targets \ny_hat = model.predict(X)\n\n# Specific example\nfrom sklearn.linear_model import \\\n  LinearRegression\nmodel = LinearRegression(n_jobs=10)\nmodel.fit(X, y)\ny_hat = model.predict(X)"
  },
  {
    "objectID": "slides/1-intro/lecture4.html#evaluation",
    "href": "slides/1-intro/lecture4.html#evaluation",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Evaluation",
    "text": "Evaluation\nHow well does the model perform?\nTo answer this we need a metric that quantifies performance.\n\n\n\nClassification:\n\n‚ÄúHow many did we label correctly?‚Äù\n\nUseful metrics:\n\nAccuracy\nPrecision\nRecall\nROC curve\n\n\n\n\nRegression:\n\n‚ÄúHow close did we get?‚Äù\n\nUseful metrics:\n\nMean squared error (MSE)\nRoot mean squared error (RMSE)\nMean absolute error (MAE)\n\n\n\n\nConsult scikit-learn docs for more metrics and description"
  },
  {
    "objectID": "slides/1-intro/lecture4.html#the-confusion-matrix",
    "href": "slides/1-intro/lecture4.html#the-confusion-matrix",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "The confusion matrix",
    "text": "The confusion matrix\nThe outcomes of our predictions can be illustrated by the confusion matrix:\n\n\n\nBinary classification case\n\n\nThe different classification performance metrics can all be defined from these entries"
  },
  {
    "objectID": "slides/1-intro/lecture4.html#metrics-recap",
    "href": "slides/1-intro/lecture4.html#metrics-recap",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Metrics recap",
    "text": "Metrics recap\n\nAccuracy:\n‚ÄúHow many did we get right?‚Äù\n\\(\\small\\mathrm{accuracy} = \\frac{\\mathrm{\\color{Green}{TP + TN}}}{\\mathrm{\\color{Purple}{TP + TN + FP + FN}}}\\)\n\n\n\n\n\nPrecision:\n‚ÄúHow many false positives did we avoid?‚Äù\n\\(\\small\\mathrm{precision} = \\frac{\\mathrm{\\color{Green}{TP}}}{\\mathrm{\\color{Purple}{TP + FP}}}\\)\n\n\n\n\n\nRecall:\n‚ÄúHow many true positives did we get right?‚Äù\n\\(\\small\\mathrm{recall} = \\frac{\\mathrm{\\color{Green}{TP}}}{\\mathrm{\\color{Purple}{TP + FN}}}\\)"
  },
  {
    "objectID": "slides/1-intro/lecture4.html#computing-metrics",
    "href": "slides/1-intro/lecture4.html#computing-metrics",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Computing metrics",
    "text": "Computing metrics\nscikit-learn has implemented a long list of metrics\n\n\n\n&gt;&gt;&gt; from sklearn.metrics import accuracy_score, confusion_matrix\n&gt;&gt;&gt; y_pred = [0, 0, 1, 0, 1, 1]\n&gt;&gt;&gt; y_true = [0, 0, 0, 1, 0, 1]\n&gt;&gt;&gt; accuracy_score(y_true, y_pred)\n0.5\n&gt;&gt;&gt; confusion_matrix(y_true, y_pred)\narray([[2, 2],\n       [1, 1]])\n&gt;&gt;&gt; \nConfusion matrix can be plotted easily using ConfusionMatrixDisplay"
  },
  {
    "objectID": "slides/1-intro/lecture4.html#setting-thresholds",
    "href": "slides/1-intro/lecture4.html#setting-thresholds",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Setting thresholds",
    "text": "Setting thresholds\n\n\n\n\n\nGoogle for Developers: Accuracy, precision, recall"
  },
  {
    "objectID": "slides/1-intro/lecture4.html#multiclass-classification",
    "href": "slides/1-intro/lecture4.html#multiclass-classification",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Multiclass classification",
    "text": "Multiclass classification\nThings change a bit when we consider tasks with more than two classes\n\n\nAccuracy: Still means ratio of correct predictions, but let‚Äôs redefine as\n\\[\n  \\small\n  \\mathrm{accuracy}(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} \\unicode{x1D7D9}(y = \\hat{y}), \\qquad\n  \\unicode{x1D7D9}(i,j) = \\begin{cases} 1, & i = j \\\\ 0, & i \\neq j \\end{cases}\n  \\]\n\n\n\n\nPrecision and recall: Only make sense in binary setting, but:\n\nWe can compute these for ‚Äúone class vs the rest‚Äù, and average results for each class\n\n\n\n\n\nConfusion matrix: No change"
  },
  {
    "objectID": "slides/1-intro/lecture4.html#train-test-split",
    "href": "slides/1-intro/lecture4.html#train-test-split",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Train-test split",
    "text": "Train-test split\nEvaluating a model should be done on an independent data set (we want to know how well it performs on new, unseen data)\nTypically we set aside a part of the data, and use this only for final evaluation.\n\n\n\n\n\n# \"X\" are the data, \"y\" are the targets.\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\nMore in DAT158-1.3-Binary_classification.ipynb"
  },
  {
    "objectID": "slides/1-intro/lecture4.html#train-validation-test-split",
    "href": "slides/1-intro/lecture4.html#train-validation-test-split",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Train-validation-test split",
    "text": "Train-validation-test split\nModel selection\n\nIn case we want to compare different models, we need a third set:  The validation set\nThe test set is still only for final evaluation\n\n\n\n\n\n\n\nML models are prone to overfitting ‚Äì i.e.¬†memorising the training data.\nHow do we know if (when) this happens?\n\nCan compare performance on the training set to the validation set"
  },
  {
    "objectID": "slides/1-intro/lecture4.html#cross-validation",
    "href": "slides/1-intro/lecture4.html#cross-validation",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Cross-validation",
    "text": "Cross-validation\nWe can do even better estimates by rotating the training data:\n\n\n\nSee scikit-learn docs"
  },
  {
    "objectID": "slides/1-intro/lecture4.html#cross-validation-cont.",
    "href": "slides/1-intro/lecture4.html#cross-validation-cont.",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Cross-validation (cont.)",
    "text": "Cross-validation (cont.)\n\n\n‚ÄúSimple‚Äù splitting is called k-fold, where k is the number of splits\nfrom sklearn.model_selection import KFold\n\nkf = KFold(n_splits=2)\nfor train, test in kf.split(X):\n    # train, test are dataset indices\n    ...\n\n\n\n\n\nWe can keep the original class distribution in each split by using stratified k-fold cross-validation\nfrom sklearn.model_selection import \\\n  StratifiedKFold\n  \nskf = StratifiedKFold(n_splits=3)\nfor train, test in skf.split(X, y):\n  ..."
  },
  {
    "objectID": "slides/1-intro/lecture4.html#piecing-it-together",
    "href": "slides/1-intro/lecture4.html#piecing-it-together",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Piecing it together",
    "text": "Piecing it together"
  },
  {
    "objectID": "slides/1-intro/lecture4.html#serving-an-ml-model",
    "href": "slides/1-intro/lecture4.html#serving-an-ml-model",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Serving an ML model",
    "text": "Serving an ML model\n\n\n\n\n\nAwesome ML workflow\n\n\n\n\n\n\n    \n\n\n\n?\n\n\n\n\n\nUser"
  },
  {
    "objectID": "slides/1-intro/lecture4.html#serving-an-ml-model-1",
    "href": "slides/1-intro/lecture4.html#serving-an-ml-model-1",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Serving an ML model",
    "text": "Serving an ML model\n\n\nOption 1: Local\nModel runs on user hardware\n\n\n\n\nPros:\n\nNo server hardware\nNo network connection needed\nData stays with user\n\n\n\n\nCons: Model may need to be\n\nCompressed\nSimplified\nEncrypted\n\n\n\n\n\n\nOption 2: Central\n\nModel runs on server, user interacts via webpage / API\n\n\n\nPros:\n\nNo limits on model complexity\nModel updates are seamless (to user)\n\n\n\n\nCons:\n\nNeed low-latency network\nUser loses control over data"
  },
  {
    "objectID": "slides/1-intro/lecture4.html#some-hosting-services-gradio",
    "href": "slides/1-intro/lecture4.html#some-hosting-services-gradio",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Some hosting services: Gradio",
    "text": "Some hosting services: Gradio"
  },
  {
    "objectID": "slides/1-intro/lecture4.html#some-hosting-services-streamlit",
    "href": "slides/1-intro/lecture4.html#some-hosting-services-streamlit",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Some hosting services: Streamlit",
    "text": "Some hosting services: Streamlit"
  },
  {
    "objectID": "slides/1-intro/lecture4.html#mandatory-exercise-after-module-1",
    "href": "slides/1-intro/lecture4.html#mandatory-exercise-after-module-1",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Mandatory exercise after Module 1",
    "text": "Mandatory exercise after Module 1\nTest: Concepts and theory\nMultiple-choice quiz about topics from the textbook, lectures, and exercises (notebooks)\nDeadline: 14. September"
  },
  {
    "objectID": "slides/1-intro/lecture4.html#referansegruppe",
    "href": "slides/1-intro/lecture4.html#referansegruppe",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Referansegruppe",
    "text": "Referansegruppe"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#recording-from-wednesday",
    "href": "slides/1-intro/lecture2.html#recording-from-wednesday",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Recording from Wednesday",
    "text": "Recording from Wednesday\nPosted on Canvas"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#practical-information",
    "href": "slides/1-intro/lecture2.html#practical-information",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Practical information",
    "text": "Practical information\n\n\n\n\nThe ML part consists of 3 modules:\n\nIntroduction to ML\nMachine learning models\nEnd-to-end ML project\n\n\n\n\n\nTextbook: A. G√©ron: Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow, 3rd edition\n\n\n\n\nLab: Work (and get help) with exercises\n\nBergen: Wednesdays 12.15, E403 & E443\nF√∏rde / Haugesund: Mondays 12.15 / 13.15\n\n\n\n\n\nMandatory assignments:\n\nMultiple choice test after module 1\nProject work after module 3 (but feel free to start early!)\n\n\n\n\n\n\n\n\nA. G√©ron: Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow, 3rd edition\n\n\n\n\n\nGeneral info:\n\nAnnouncements on Canvas\nDiscord"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#proposed-oblig-dates",
    "href": "slides/1-intro/lecture2.html#proposed-oblig-dates",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Proposed oblig dates",
    "text": "Proposed oblig dates\n\n\n\n\n14. September: Quiz üéì \n\n\n4. October: Competition üèÖ (not mandatory)‚ÄÉ\n\n\n2. November: Project hand-in üìú"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#programming",
    "href": "slides/1-intro/lecture2.html#programming",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Programming",
    "text": "Programming\n\n\nThe machine learning part of the course is based on the python programming language\nFree resources for learning python are posted on Canvas, such as\n\nKaggle Learning\nGoogle Edu\n\n\n\n\nTwo options for running python:\n\nUse cloud services, such as\n\nGoogle Colab\nKaggle\nDeepNote\n\nInstall locally"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#the-5-min-python-tutorial",
    "href": "slides/1-intro/lecture2.html#the-5-min-python-tutorial",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "The 5 min Python tutorial",
    "text": "The 5 min Python tutorial\n\n\n\n\n\nDeclare a variable:\nx = 5\nprint(x)  # prints \"5\"\n\n\n\n\nUseful data structures:\n\nlists:\n\nmylist = [1, 'a', 2.7]  # can mix types\n\nlen(mylist)  # returns 3\n\nfor element in mylist:  # loop over contents\n  print(element)\n\n\n\ndictionaries:\n\nmydict = {'a': 1, 'b': 2}\n\nfor key in mydict:\n  print('key:', key, 'value:', mydict[key])"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#the-5-min-python-tutorial-1",
    "href": "slides/1-intro/lecture2.html#the-5-min-python-tutorial-1",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "The 5 min Python tutorial",
    "text": "The 5 min Python tutorial\n\n\n\n\nDefine a function:\ndef greeting(name):\n  result = 'Hello ' + name + '!'\n  return result \n\nprint(greeting('World'))\n# prints \"Hello World!\"\n\nFunctions with multiple arguments and multiple outputs:\ndef greeting(name, title):\n  result = 'Hello ' + title + ' ' + name + '!'\n  length = len(result)\n  return [result, length]\n\n# Input arguments by order\ntext, length = greeting('Frankenstein', 'Dr')\n\n# Input arguments by name\ntext, length = greeting(title='Mr', name='Bond')"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#the-5-min-python-tutorial-2",
    "href": "slides/1-intro/lecture2.html#the-5-min-python-tutorial-2",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "The 5 min Python tutorial",
    "text": "The 5 min Python tutorial\nConditionals:\ndef is_number_five(number):\n\n  if not isinstance(number, int):              # Type checking\n    raise TypeError('This is not an integer')  # Errors can be caught and handled \n\n  if number &gt; 5:\n    print('Too high')\n    \n  elif number &lt; 5:\n    print('Too low')\n    \n  elif number == 5:\n    print('Number is 5')\n\n  else:\n    pass  # Unlikely to end up here"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#the-5-min-python-tutorial-3",
    "href": "slides/1-intro/lecture2.html#the-5-min-python-tutorial-3",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "The 5 min Python tutorial",
    "text": "The 5 min Python tutorial\nReserved keywords you cannot redefine:\n\n\nFalse\nNone\nTrue\nand\nas\nassert\nasync\n\nawait\nbreak\nclass\ncontinue\ndef\ndel\nelif\n\nelse\nexcept\nfinally\nfor\nfrom\nglobal\nif\n\nimport\nin\nis\nlambda\nnonlocal\nnot\nor\n\npass\nraise\nreturn\ntry\nwhile\nwith\nyield\n\nUseful built-in functions you should not redefine:\n\n\nabs()\naiter()\nall()\nanext()\nany()\nascii()\nbin()\nbool()\nbreakpoint()\nbytearray()\nbytes()\ncallable()\nchr()\nclassmethod()\n\ncompile()\ncomplex()\ndelattr()\ndict()\ndir()\ndivmod()\nenumerate()\neval()\nexec()\nfilter()\nfloat()\nformat()\nfrozenset()\ngetattr()\n\nglobals()\nhasattr()\nhash()\nhelp()\nhex()\nid()\ninput()\nint()\nisinstance()\nissubclass()\niter()\nlen()\nlist()\nlocals()\n\nmap()\nmax()\nmemoryview()\nmin()\nnext()\nobject()\noct()\nopen()\nord()\npow()\nprint()\nproperty()\nrange()\nrepr()\n\nreversed()\nround()\nset()\nsetattr()\nslice()\nsorted()\nstaticmethod()\nstr()\nsum()\nsuper()\ntuple()\ntype()\nvars()\nzip()"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#the-5-min-python-tutorial-4",
    "href": "slides/1-intro/lecture2.html#the-5-min-python-tutorial-4",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "The 5 min Python tutorial",
    "text": "The 5 min Python tutorial\n\n\nImporting code from libraries:\n# Import everything\nimport math\nmath.pi\n# returns 3.141592653589793\n\n# Import just what you need\nfrom math import pi\npi\n# returns 3.141592653589793\n\n# Import it under a different name\nfrom math import pi as coolnumber\ncoolnumber\n# returns 3.141592653589793\n\n\n\n\nExternal libraries needed for our exercises are already installed on Colab, and listed in requirements.txt if you want to install them locally\n\nscikit-learn    # ML models and utilities\nnumpy           # Advanced arrays \nscipy           # Scientific functions\nnotebook        # Jupyter notebooks\nmatplotlib      # Plots and visualisation\nseaborn         # Even cooler plots \ngradio          # Turn ML models into an app"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#notebooks",
    "href": "slides/1-intro/lecture2.html#notebooks",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Notebooks",
    "text": "Notebooks\nExercises are given in form of Jupyter notebooks\n\nCan mix code, results, and notes (markdown and TeX) in the same file\nThese are partially filled, and you fill the rest\nCan be run locally or in cloud services"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#section-2",
    "href": "slides/1-intro/lecture2.html#section-2",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "",
    "text": "git clone git@github.com:HVL-ML/DAT158.git"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#python-libraries",
    "href": "slides/1-intro/lecture2.html#python-libraries",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Python libraries",
    "text": "Python libraries\n\n\n\n\n\n\n\n\nhttps://numpy.org/\n\n\nnumpy provides fast manipulation of large arrays and matrices\n\n\n\n\n\n\nhttps://scikit-learn.org/\n\n\nscikit-learn has a big selection of machine learning models and functions for data processing and evaluation"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#numpy-arrays",
    "href": "slides/1-intro/lecture2.html#numpy-arrays",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "numpy arrays",
    "text": "numpy arrays\nThe core of numpy is the array, on which we can do operations without explicit loops:\nExample: Given a list of numbers, make a new list, where all the elements are multiplied by 2.\n\n\nVanilla python:\n&gt;&gt;&gt; a = [1,2,3]; b = []\n&gt;&gt;&gt; for elem in a:\n&gt;&gt;&gt;   b.append(elem * 2)\nb\n[2, 4, 6]\n\n\nNumpy:\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; a = np.array([1,2,3])\n&gt;&gt;&gt; b = a * 2\n&gt;&gt;&gt; b\narray([2 4 6])\n\n\n\nArrays can have any number of dimensions ‚Äì e.g.¬†a 2D matrix is written\n&gt;&gt;&gt; a = np.array([[1,2,3],[4,5,6]])\n&gt;&gt;&gt; print(a)\n[[1 2 3]\n [4 5 6]]\n&gt;&gt;&gt; a.ndim\n2\n&gt;&gt;&gt; a.shape\n(2, 3)"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#numpy-arrays-1",
    "href": "slides/1-intro/lecture2.html#numpy-arrays-1",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "numpy arrays",
    "text": "numpy arrays\nIn normal python, elements are sliced from a list using [start : stop]:\n&gt;&gt;&gt; a = [0, 1, 2, 3, 4]\n&gt;&gt;&gt; a[2:4]                  # (remember zero-based indexing)\n[2, 3]    \n&gt;&gt;&gt; a[:]                    # no start or stop -&gt; select everything\n[0, 1, 2, 3, 4]\n\nNumpy extends this to any number of dimensions, separated by ,\n&gt;&gt;&gt; a = np.arange(1,10).reshape(3,3)\n&gt;&gt;&gt; a\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\n\nSelect single element:\n&gt;&gt;&gt; a[1, 1]\n5\n\n\n\nSelect row:\n&gt;&gt;&gt; a[1, :]\narray([4, 5, 6])\n\n\n\nSelect column:\n&gt;&gt;&gt; a[:, 1]\narray([2, 5, 8])"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#numpy-arrays-2",
    "href": "slides/1-intro/lecture2.html#numpy-arrays-2",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "numpy arrays",
    "text": "numpy arrays\nOperations on arrays are typically done element-by-element.\n\n\n&gt;&gt;&gt; a = np.array([1,2,3])\n&gt;&gt;&gt; np.power(a, 2)\narray([1, 4, 9])\n\n\n&gt;&gt;&gt; b = np.array([4,5,6])\n&gt;&gt;&gt; a * b\narray([ 4, 10, 18])\n\n\n\nIn cases the shapes of two arrays don‚Äôt match, numpy will try to make them match(aka broadcasting):\n# I type this\n&gt;&gt;&gt; a * 2\narray([2, 4, 6])\n# ...and numpy does this:\n&gt;&gt;&gt; a * np.array([2, 2, 2])\narray([2, 4, 6])\n\n\n\n\n\nNumpy docs"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#machine-learning",
    "href": "slides/1-intro/lecture2.html#machine-learning",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Machine learning",
    "text": "Machine learning\n\n\n\n\n\n(Fig. 1-2)"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#what-is-machine-learning",
    "href": "slides/1-intro/lecture2.html#what-is-machine-learning",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n(One) definition:\n\n\n\nA computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\n\nTom Mitchell, 1997"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#different-types-of-machine-learning",
    "href": "slides/1-intro/lecture2.html#different-types-of-machine-learning",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Different types of machine learning",
    "text": "Different types of machine learning\n\n\nSupervised learning:\nEach datapoint is assigned to a label, which the model tries to predict.\nLabel is a\n\nclass (categorical): we are doing classification\nvalue (numerical): we are doing regression"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#different-types-of-machine-learning-1",
    "href": "slides/1-intro/lecture2.html#different-types-of-machine-learning-1",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Different types of machine learning",
    "text": "Different types of machine learning\n\n\nSupervised learning:\n\nEach datapoint is assigned to a label, which the model tries to predict.\n\n\n\nUnsupervised learning:\nDatapoints are unlabelled, but the model tries to group similar ones, or otherwise learn a pattern.\n\n\n\nReinforcement learning:\nThe model (aka agent) interacts with an environment, and receives rewards or penalties depending on its actions\n\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\nLunar lander"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#some-types-of-data",
    "href": "slides/1-intro/lecture2.html#some-types-of-data",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "(Some) types of data",
    "text": "(Some) types of data\n\n\n\n\nText\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n\nTime series (IPPC 2021)\n\n\n\n\n\n\n\nAudio\n\n\n\n\n\n\n\nTabular (UCI Bank)\n\n\n\n\n\n\n\nImages"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#about-data",
    "href": "slides/1-intro/lecture2.html#about-data",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "About data",
    "text": "About data\n\nStructured data can be represented in a table:\n\n\nTitanic dataset (from Kaggle)\n\n\nId\nSurvived\nPclass\nName\nSex\nAge\n...\n\n\n\n\n1\n0\n3\nBraund, Mr.¬†Owen Harris\nmale\n22\n\n\n\n2\n1\n1\nCumings, Mrs.¬†John Bradley\nfemale\n38\n\n\n\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n-\n\n\n\n4\n1\n1\nFutrelle, Mrs.¬†Jacques Heath\nfemale\n35\n\n\n\n\n\n\n‚ãÆ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfeatures \n\n\n\nbinary\n\n\ncategorical\n\n\nmissing data\n\n\n\n\n\n\n\ndata point (\\(\\mathbf{x}_2\\))\n\n\n\nData are often expressed in mathematical notation as an \\(N \\times M\\)-matrix \\(\\mathbf{X}\\), where\n\n\\(N\\) is the number of data points\n\\(M\\) is the number of features\n\nA single data point is then a vector \\(\\mathbf{x} = (x_1, \\dots, x_M)\\)."
  },
  {
    "objectID": "slides/1-intro/lecture2.html#sources-of-data",
    "href": "slides/1-intro/lecture2.html#sources-of-data",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Sources of data",
    "text": "Sources of data\n\n\nOpenly available datasets well suited for machine learning:\n\n\nKaggle Datasets\n\n\n\n\nUCI Machine Learning Repository\n\n\n\n\nGoogle Dataset Search\n\n\n\n\nPapersWithCode Datasets\n\n\n\n\nHugging Face datasets\n\n\n\n\nStatistisk Sentralbyr√•\n\n(More listed in Chap 2 in the textbook)"
  },
  {
    "objectID": "slides/1-intro/lecture2.html#data-challenges",
    "href": "slides/1-intro/lecture2.html#data-challenges",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Data challenges",
    "text": "Data challenges\n\n\nToo little data available: Can lead to overfitting: the model gives perfect predictions on known data, but does not generalise to new data\n\n\n\n\n\nUnrepresentative data: New data does not look like known data\n\n\n\n\n\n\n\nG√©ron Fig. 1-20\n\n\n\n\n\n\n\nG√©ron Fig. 1-22\n\n\n\n\n\n\nPoor quality: Noise, typing errors, missing entries, rounding errors, ‚Ä¶\n\n\n\n\n\n\n\n\nIrrelevant data: Little or no correlation with the observable we want to predict\n\n\n\n\nBias: Data contains correlations you didn‚Äôt want (see e.g.¬†stories about COMPAS, Amazon)\n\n\n\n\n\n\nüïí Very often, most of the time spent on an ML project goes to collecting, cleaning and preparing data."
  },
  {
    "objectID": "slides/1-intro/lecture2.html#machine-learning-engineering",
    "href": "slides/1-intro/lecture2.html#machine-learning-engineering",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Machine learning engineering",
    "text": "Machine learning engineering\n\n\n        \n\n\n\ntheoretical\n\n\napplied\n\n\n\n\nMachine learning models\n\n  \n\n\n\n\nMachine learning engineering\n \n\n\n\n\n\n\n\nData and society"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DAT158 Lecture notes",
    "section": "",
    "text": "Published after each lecture week ‚Äì see Canvas for additional material."
  },
  {
    "objectID": "index.html#module-1-introduction-to-machine-learning",
    "href": "index.html#module-1-introduction-to-machine-learning",
    "title": "DAT158 Lecture notes",
    "section": "Module 1: Introduction to machine learning",
    "text": "Module 1: Introduction to machine learning\nWednesday week 35\nFriday week 35\nWednesday week 36\nFriday week 36"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#ml-engineering-advanced-algorithms",
    "href": "slides/1-intro/lecture1.html#ml-engineering-advanced-algorithms",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "ML engineering ü§ù Advanced algorithms",
    "text": "ML engineering ü§ù Advanced algorithms"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#practical-information",
    "href": "slides/1-intro/lecture1.html#practical-information",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Practical information",
    "text": "Practical information\n\n\n\n\nThe ML part consists of 3 modules:\n\nIntroduction to ML\nMachine learning models\nEnd-to-end ML project\n\n\n\n\n\nTextbook: A. G√©ron: Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow, 3rd edition\n\n\n\n\nLab: Work (and get help) with exercises\n\nBergen: Wednesdays 12:15, E403 & E443\nF√∏rde / Haugesund: Mondays 12.15 / 13.15\n\n\n\n\n\nMandatory assignments:\n\nMultiple choice test after module 1\nProject work after module 3 (but feel free to start early!)\n\n\n\n\n\n\n\n\nA. G√©ron: Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow, 3rd edition\n\n\n\n\n\nGeneral info:\n\nAnnouncements on Canvas\nDiscord"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#programming",
    "href": "slides/1-intro/lecture1.html#programming",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Programming",
    "text": "Programming\n\n\nThe machine learning part of the course is based on the python programming language\nFree resources for learning python are posted on Canvas, such as\n\nKaggle Learning\nGoogle Edu\n\n\n\n\nTwo options for running python:\n\nUse cloud services, such as\n\nGoogle Colab\nKaggle\nDeepNote\n\nInstall locally"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#python-libraries",
    "href": "slides/1-intro/lecture1.html#python-libraries",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Python libraries",
    "text": "Python libraries\n\n\nnumpy allows for simple vectorisation of arrays and matrices\n# Square ten numbers\nimport numpy as np\nnumbers = np.arange(start=0, stop=11)\nsquared_numbers = numbers ** 2  \nprint(squared_numbers)\n(compare to Java:)\npublic class Main {\n  public static void main(String[] args) {\n    int[] numbers = new int[11];\n    for (int i = 0; i &lt;= 10; i++) {\n      numbers[i] = i * i;\n      System.out.println(numbers[i]);\n    }\n  }\n}"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#python-libraries-1",
    "href": "slides/1-intro/lecture1.html#python-libraries-1",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Python libraries",
    "text": "Python libraries\n\n\nnumpy allows for simple vectorisation of arrays and matrices\n# Square ten numbers\nimport numpy as np\nnumbers = np.arange(start=0, stop=11)\nsquared_numbers = numbers ** 2  # no for-loop\nprint(squared_numbers)\n(compare to Java:)\npublic class Main {\n  public static void main(String[] args) {\n    int[] numbers = new int[11];\n    for (int i = 0; i &lt;= 10; i++) {\n      numbers[i] = i * i;\n      System.out.println(numbers[i]);\n    }\n  }\n}\n\n\nscikit-learn has a big selection of machine learning models and functions for data processing and evaluation"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#notebooks",
    "href": "slides/1-intro/lecture1.html#notebooks",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Notebooks",
    "text": "Notebooks\nExercises are given in form of Jupyter notebooks\n\nCan mix code, results, and notes (markdown and TeX) in the same file\nThese are partially filled, and you fill the rest\nCan be run locally or in cloud services"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#goals-for-the-ml-part-of-the-course",
    "href": "slides/1-intro/lecture1.html#goals-for-the-ml-part-of-the-course",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Goals for the ML part of the course",
    "text": "Goals for the ML part of the course\n\nWhat we will learn:\n\nCentral concepts in ML and data analysis\nDetails of a selection of ML models\nWorkflow for setting up a complete ML-based software product\n\n\n\n\nWhat we will not learn:\n\nNeural networks and more advanced models based on these\nModels taking unstructured data (images, audio, text) as input\nGenerative models (LLMs, diffusion models, ‚Ä¶)\n\n\n\n DAT255 Deep learning engineering"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#goals-for-module-1-this-week-and-next",
    "href": "slides/1-intro/lecture1.html#goals-for-module-1-this-week-and-next",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Goals for module 1 (this week and next)",
    "text": "Goals for module 1 (this week and next)\n\n\n\n\nGet comfortable with python, including the libraries numpy and scikit-learn\n\n\n\n\n\nOverview of ML and fundamental concepts: Chap 1 of the textbook (The Machine Learning landscape), available here\n\n\n\n\n\nWhat are the parts of a machine learning project Chap 2 (End-to-End Machine Learning Project)\n\n\n\n\n\nClassifying data Chap 3 (Classification)"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#examples-of-machine-learning",
    "href": "slides/1-intro/lecture1.html#examples-of-machine-learning",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "üëì Examples of machine learning",
    "text": "üëì Examples of machine learning"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#section-12",
    "href": "slides/1-intro/lecture1.html#section-12",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "",
    "text": "Fan, W., Yang, Y., Qi, J. et al., Nat Commun 15, 1347 (2024)"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#section-13",
    "href": "slides/1-intro/lecture1.html#section-13",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "",
    "text": "Recommender system"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#eliza-1966",
    "href": "slides/1-intro/lecture1.html#eliza-1966",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "ELIZA (1966)",
    "text": "ELIZA (1966)\nA chatbot based on symbolic AI\n\n\n\n\nsynon: be am is are was\nkey: xnone\n  decomp: *\n    reasmb: I'm not sure I understand you fully.\n    reasmb: Please go on.\n    reasmb: What does that suggest to you ?\n    reasmb: Do you feel strongly about discussing such things ?\nkey: sorry\n  decomp: *\n    reasmb: Please don't apologise.\n    reasmb: Apologies are not necessary.\n    reasmb: I've told you that apologies are not required.\nkey: apologise\n  decomp: *\n    reasmb: goto sorry\nkey: remember 5\n  decomp: * i remember *\n    reasmb: Do you often think of (2) ?\n    reasmb: Does thinking of (2) bring anything else to mind ?\n    reasmb: What else do you recollect ?\n    reasmb: Why do you recollect (2) just now ?\n    reasmb: What in the present situation reminds you of (2) ?\n    reasmb: What is the connection between me and (2) ?\n  decomp: * do you remember *\n    reasmb: Did you think I would forget (2) ?\n    reasmb: Why do you think I should recall (2) now ?\n    reasmb: What about (2) ?\n    reasmb: goto what\n    reasmb: You mentioned (2) ?\nkey: if 3\n  decomp: * if *\n    reasmb: Do you think its likely that (2) ?\n    reasmb: Do you wish that (2) ?\n    reasmb: What do you know about (2) ?\n    reasmb: Really, if (2) ?\nkey: dreamed 4\n  decomp: * i dreamed *\n    reasmb: Really, (2) ?\n    reasmb: Have you ever fantasized (2) while you were awake ?\n    reasmb: Have you ever dreamed (2) before ?\n    reasmb: goto dream\nkey: dream 3\n  decomp: *\n    reasmb: What does that dream suggest to you ?\n    reasmb: Do you dream often ?\n    reasmb: What persons appear in your dreams ?\n    reasmb: Do you believe that dreams have something to do with your problems ?\nkey: perhaps\n  decomp: *\n    reasmb: You don't seem quite certain.\n    reasmb: Why the uncertain tone ?\n    reasmb: Can't you be more positive ?\n    reasmb: You aren't sure ?"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#traditional-rule-based-programming",
    "href": "slides/1-intro/lecture1.html#traditional-rule-based-programming",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Traditional (rule-based) programming",
    "text": "Traditional (rule-based) programming\n\n\n\n\n(Fig. 1-1 in Hands-On ML)"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#machine-learning",
    "href": "slides/1-intro/lecture1.html#machine-learning",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Machine learning",
    "text": "Machine learning\n\n\n\n\n\n(Fig. 1-2)"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#ml-vs-ai",
    "href": "slides/1-intro/lecture1.html#ml-vs-ai",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "ML vs AI",
    "text": "ML vs AI\n\n\n\nWikiMedia Commons"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#what-is-machine-learning",
    "href": "slides/1-intro/lecture1.html#what-is-machine-learning",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n(One) definition:\n\n\n\nA computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\n\nTom Mitchell, 1997"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#when-should-we-use-machine-learning",
    "href": "slides/1-intro/lecture1.html#when-should-we-use-machine-learning",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "When should we use machine learning?",
    "text": "When should we use machine learning?\n\n\n\nML is good when we have access data, which is\n\ndescriptive of the task we want to solve\navailable in big¬π amounts\n\n\n[1] (We will get back to how much this is)\n\n\n\nML is less useful when\n\nthe data are incomplete or inconsistent\nthe data have simple correlations\nthe underlying (data generating) process can be modelled analytically\n\n\n\n\n\n\nML is bad when\n\nwe need an explainable / interpretable model\nunder certain requirements of privacy\nthe data change (in an unforeseen way) over time\nthe developer don‚Äôt know what they are doing"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#different-types-of-machine-learning",
    "href": "slides/1-intro/lecture1.html#different-types-of-machine-learning",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Different types of machine learning",
    "text": "Different types of machine learning\n\n\nSupervised learning:\nEach datapoint is assigned to a label, which the model tries to predict.\nLabel is a\n\nclass (categorical): we are doing classification\nvalue (numerical): we are doing regression"
  },
  {
    "objectID": "slides/1-intro/lecture1.html#different-types-of-machine-learning-1",
    "href": "slides/1-intro/lecture1.html#different-types-of-machine-learning-1",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Different types of machine learning",
    "text": "Different types of machine learning\n\n\nSupervised learning:\n\nEach datapoint is assigned to a label, which the model tries to predict.\n\n\n\nUnsupervised learning:\nDatapoints are unlabelled, but the model tries to group similar ones, or otherwise learn a pattern.\n\n\n\nReinforcement learning:\nThe model (aka agent) interacts with an environment, and receives rewards or penalties depending on its actions\n\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\nLunar lander"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#assignment-1-out",
    "href": "slides/1-intro/lecture3.html#assignment-1-out",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Assignment 1 out",
    "text": "Assignment 1 out\n\n\nDeadline: 14. September"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#machine-learning-engineering",
    "href": "slides/1-intro/lecture3.html#machine-learning-engineering",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Machine learning engineering",
    "text": "Machine learning engineering\n\n\n        \n\n\n\ntheoretical\n\n\napplied\n\n\n\n\nMachine learning models\n\n  \n\n\n\n\nMachine learning engineering\n\n\n\n\nData and society"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#machine-learning-engineering-1",
    "href": "slides/1-intro/lecture3.html#machine-learning-engineering-1",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Machine learning engineering",
    "text": "Machine learning engineering\n\n\n\n\n\n\n\n\n\nStudy the problem üîç\n\nExploratory data analysis\nDomain knowledge\nData preparation\n\n\n\n\n\nTrain ML model üíª\n\nAlgorithm/software development\n\n\n\n\n\n\nEvaluate üìà\n\nMetric selection\nModel selection\nUncertainty quantification\n\n\n\n\n\nLaunch üöÄ / Analyse errors ‚ùå\n\nDeployment\nDiagnostics and visualisation"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#exploratory-data-analysis",
    "href": "slides/1-intro/lecture3.html#exploratory-data-analysis",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nGet to know the data ‚Äì structure, feature types, etc.\nLook for correlations\n\n\n\n\n\n\nIris flowers\n\n\n(notebooks/DAT158-1.1-Simple_examples.ipynb)\n\n\n\n\nPairwise feature scatter plots"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#data-preparation",
    "href": "slides/1-intro/lecture3.html#data-preparation",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Data preparation",
    "text": "Data preparation\nMany (most) real-world datasets need some cleaning.\nTypical problems, and various solutions:\n\n\n\n\nMissing data\n\nFill with some value (zero, mean, ‚Ä¶) aka imputation\nSkip datapoints containing missing data\nSkip entire feature containing missing data\n\n\n\n\n\n\nText attributes\n\nConvert to categorical values\n\n\n\nocean_proximity\n\ncategorical\n\n\n\n\n\"NEAR BAY\"\n\n0\n\n\n\"INLAND\"\n\n1\n\n\n\"NEAR OCEAN\"\n\n2"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#data-preparation-cont.",
    "href": "slides/1-intro/lecture3.html#data-preparation-cont.",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Data preparation (cont.)",
    "text": "Data preparation (cont.)\nMany (most) real-world datasets need some cleaning\nTypical problems, and various solutions:\n\n\n\n\nFeature values have different scales\n\nNormalise values (shift to a range [0, 1])\nStandardize values (shift to have mean equal to 0 and variance equal to 1)\n\n\n\n\n\n\nFeature values are not normally distributed\n\nTransform values (compute e.g.¬†the logarithm)\n\n\n\n\n\nWikiMedia Commons\n\n\n\n\n\nSee notebooks/DAT158-1.5-Regression.ipynb"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#model-training",
    "href": "slides/1-intro/lecture3.html#model-training",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Model training",
    "text": "Model training\nDetails of different ML models is the topic for Module 2\nFor now, let‚Äôs represent a generic ML model as a function \\(\\color{Purple}{f}\\):\n\n\\[\n\\large \\hat{\\color{DarkBlue}{y}} = \\color{Purple}{f}(\\color{DarkOrange}{\\mathbf{x}}, \\boldsymbol{\\color{teal}{\\theta}})\n\\]\n\nwhere\n\n\\(\\hat{\\color{DarkBlue}{y}}\\) is the prediction\n\\(\\color{DarkOrange}{\\mathbf{x}}\\)¬†is a data point\n\\(\\boldsymbol{\\color{teal}{\\theta}}\\) are parameters of the model\n\n\nTraining is the process of finding the best parameters \\(\\boldsymbol{\\color{teal}{\\theta}}\\)¬†so that the prediction \\(\\hat{\\color{DarkBlue}{y}}\\) is as close as possible to the known target value \\(\\color{DarkBlue}{y}\\)."
  },
  {
    "objectID": "slides/1-intro/lecture3.html#model-training-1",
    "href": "slides/1-intro/lecture3.html#model-training-1",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Model training",
    "text": "Model training\nDetails of different ML models is the topic for Module 2\nFor now, let‚Äôs represent a generic ML model as a function \\(\\color{Purple}{f}\\):\n\n\\[\n\\large \\hat{\\color{DarkBlue}{y}} = \\color{Purple}{f}(\\color{DarkOrange}{\\mathbf{x}}, \\boldsymbol{\\color{teal}{\\theta}})\n\\]\n\n\nTraining is the process of finding the best parameters \\(\\boldsymbol{\\color{teal}{\\theta}}\\)¬†so that the prediction \\(\\hat{\\color{DarkBlue}{y}}\\) is as close as possible to the known target value \\(\\color{DarkBlue}{y}\\).\n\nscikit-learn API:\n\n\n# Generic model API\nfrom sklearn.model_family import ModelName\n\nmodel = ModelName(some_setting=\"foo\")\nmodel.fit(X, y) # X are data, y are targets \ny_hat = model.predict(X)\n\n# Specific example\nfrom sklearn.linear_model import \\\n  LinearRegression\nmodel = LinearRegression(n_jobs=10)\nmodel.fit(X, y)\ny_hat = model.predict(X)"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#evaluation",
    "href": "slides/1-intro/lecture3.html#evaluation",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Evaluation",
    "text": "Evaluation\nHow well does the model perform?\nTo answer this we need a metric that quantifies performance.\n\n\n\nClassification:\n\n‚ÄúHow many did we label correctly?‚Äù\n\nUseful metrics:\n\nAccuracy\nPrecision\nRecall\nROC curve\n\n\n\n\nRegression:\n\n‚ÄúHow close did we get?‚Äù\n\nUseful metrics:\n\nMean squared error (MSE)\nRoot mean squared error (RMSE)\nMean absolute error (MAE)\n\n\n\n\nConsult scikit-learn docs for more metrics and description"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#binary-classification",
    "href": "slides/1-intro/lecture3.html#binary-classification",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Binary classification",
    "text": "Binary classification\nIn DAT158-1.3-Binary_classification.ipynb we explore a task with two outcomes:\n\nPatient has diabetes (1 / True)\nPatient does not have diabetes (0 / False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nindex\npregnancies\nglucose\ndiastolic\ntriceps\ninsulin\nbmi\ndpf\nage\ndiabetes\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n\n(More about the data here)\nWe want our ML algorithm \\(\\color{Purple}{f}\\) to predict no/yes for new patients with observations \\(\\color{DarkOrange}{\\mathbf{x}}\\),\n\\[\n\\color{Purple}{f}: \\color{DarkOrange}{\\mathbf{x}} \\in \\mathbb{R}^n \\longrightarrow \\{0,1\\} \\,.\n\\]"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#the-confusion-matrix",
    "href": "slides/1-intro/lecture3.html#the-confusion-matrix",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "The confusion matrix",
    "text": "The confusion matrix\nThe outcomes of our predictions can be illustrated by the confusion matrix:\n\n\n\nBinary classification case\n\n\nThe different classification performance metrics can all be defined from these entries"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#accuracy",
    "href": "slides/1-intro/lecture3.html#accuracy",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Accuracy",
    "text": "Accuracy\nAccuracy measures how many samples are classified correctly, relative to the total number of samples:\n\\[\n\\small\n\\mathrm{accuracy} = \\frac{\\mathrm{correct\\; classifications}}{\\mathrm{all\\; classifications}}\n  = \\frac{\\mathrm{\\color{Green}{TP + TN}}}{\\mathrm{\\color{Purple}{TP + TN + FP + FN}}}\n\\]"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#precision",
    "href": "slides/1-intro/lecture3.html#precision",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Precision",
    "text": "Precision\nPrecision measures how many positive classifications that are actually positive:\n\\[\n\\small\n\\mathrm{precision} = \\frac{\\mathrm{correct\\; positive\\; classifications}}{\\mathrm{all\\; positive\\; classifications}}\n  = \\frac{\\mathrm{\\color{Green}{TP}}}{\\mathrm{\\color{Purple}{TP + FP}}}\n\\]"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#recall",
    "href": "slides/1-intro/lecture3.html#recall",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Recall",
    "text": "Recall\nRecall measures how many of the actual positives that were classified as positive:\n\\[\n\\small\n\\mathrm{recall} = \\frac{\\mathrm{correct\\; positive\\; classifications}}{\\mathrm{all\\; actual\\; positives}}\n  = \\frac{\\mathrm{\\color{Green}{TP}}}{\\mathrm{\\color{Purple}{TP + FN}}}\n\\]"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#computing-metrics",
    "href": "slides/1-intro/lecture3.html#computing-metrics",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Computing metrics",
    "text": "Computing metrics\nscikit-learn has implemented a long list of metrics\n\n\n\n&gt;&gt;&gt; from sklearn.metrics import accuracy_score, confusion_matrix\n&gt;&gt;&gt; y_pred = [0, 0, 1, 0, 1, 1]\n&gt;&gt;&gt; y_true = [0, 0, 0, 1, 0, 1]\n&gt;&gt;&gt; accuracy_score(y_true, y_pred)\n0.5\n&gt;&gt;&gt; confusion_matrix(y_true, y_pred)\narray([[2, 2],\n       [1, 1]])\nConfusion matrix can be plotted easily using ConfusionMatrixDisplay"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#setting-thresholds",
    "href": "slides/1-intro/lecture3.html#setting-thresholds",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Setting thresholds",
    "text": "Setting thresholds\nMost ML classifiers actually predict a decimal number between 0 and 1, leaving us to select a threshold for where to divide between the negative and positive class.\n\\[\n\\color{Purple}{f}: \\color{DarkOrange}{\\mathbf{x}} \\longrightarrow [0, 1]\n\\]\n\n\n\n\n\n\n\n\n Most like class 0\n\n\nMost like class 1 \n\nTypically one chooses 0.5 as default, but let‚Äôs look at how changing the threshold changes the metrics."
  },
  {
    "objectID": "slides/1-intro/lecture3.html#setting-thresholds-cont.",
    "href": "slides/1-intro/lecture3.html#setting-thresholds-cont.",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Setting thresholds (cont.)",
    "text": "Setting thresholds (cont.)\n\n\n\n\n\nGoogle for Developers: Accuracy, precision, recall"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#imbalanced-data",
    "href": "slides/1-intro/lecture3.html#imbalanced-data",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Imbalanced data",
    "text": "Imbalanced data\n\n\n\n\n\nGoogle for Developers: Accuracy, precision, recall"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#precision-recall-curves",
    "href": "slides/1-intro/lecture3.html#precision-recall-curves",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Precision-recall curves",
    "text": "Precision-recall curves\nThe metrics so far are computed at a single threshold value. What if we want to evaluate a model for all possible thresholds?\n Plot precision as a function of recall:\n\n\n\nG√©ron Fig 3-6"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#receiver-operator-characteristic-roc",
    "href": "slides/1-intro/lecture3.html#receiver-operator-characteristic-roc",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Receiver Operator Characteristic (ROC)",
    "text": "Receiver Operator Characteristic (ROC)\nA more common option is to plot the true positive rate (TPR) as function of false positive rate (FPR):\n\n\n\nG√©ron Fig 3-6\n\n\n\n\\[\n\\tiny\n\\mathrm{TPR} = \\frac{\\mathrm{TP}}{\\mathrm{TP + FN}}\n\\]\n‚ÄúHow many positives did I get right‚Äù\n\n\n\\[\n\\tiny\n\\mathrm{FPR} = \\frac{\\mathrm{FP}}{\\mathrm{FP + TN}}\n\\]\n‚ÄúHow many negatives did I get wrong‚Äù\n\n\nGood models have high area-under-curve (AUC)"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#receiver-operator-characteristic-roc-1",
    "href": "slides/1-intro/lecture3.html#receiver-operator-characteristic-roc-1",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Receiver Operator Characteristic (ROC)",
    "text": "Receiver Operator Characteristic (ROC)\n\n\n\n\n\nGoogle for Developers: Accuracy, precision, recall"
  },
  {
    "objectID": "slides/1-intro/lecture3.html#quiz",
    "href": "slides/1-intro/lecture3.html#quiz",
    "title": "DAT158: Machine Learning Engineering and Advanced Algorithms",
    "section": "Quiz üß†",
    "text": "Quiz üß†\n\nYou have trained a model to classify cancer from X-ray images. Positive (1) class means cancer. Now you need to choose a classification threshold.\n\n\n\nShould you choose threshold A, B, or C?"
  }
]