{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 07.09.2023, A. S. Lundervold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HVL-ML/DAT158/blob/master/notebooks/DAT158-1.1-Simple_examples.ipynb) &nbsp; [![kaggle](https://camo.githubusercontent.com/a08ca511178e691ace596a95d334f73cf4ce06e83a5c4a5169b8bb68cac27bef/68747470733a2f2f6b6167676c652e636f6d2f7374617469632f696d616765732f6f70656e2d696e2d6b6167676c652e737667)](https://www.kaggle.com/alexanderlundervold/2023-dat158-1-1-simple-examples-ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory\n",
    "# or on Kaggle, as that makes some difference for the code below.\n",
    "# We'll do this in every notebook of the course.\n",
    "try:\n",
    "    import google.colab\n",
    "    colab=True\n",
    "except:\n",
    "    colab=False\n",
    "\n",
    "import os\n",
    "kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# A first example of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Welcome to the first step on your hands-on journey into machine learning! In this Jupyter Notebook, we'll use machine learning to study some standard benchmark data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can download the notebook from the course GitHub repo: https://github.com/HVL-ML/DAT158."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> You'll find some cells marked \"Your turn!\" below. These are meant to help you experiment with the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Tip: Always feel free to modify the code, change parameters, or experiment on your own! This hands-on exploration can lead to deeper understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This notebook aims to introduce some of the essential ingredients and techniques in machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As we've discussed, in traditional programming, we write specific instructions for a computer to follow. In machine learning, instead of providing explicit instructions, we feed data to algorithms, and the algorithms learn from this data. This notebook will make this more concrete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To get started, we need some data. Here, we'll use a much studied, classical data set: the [Iris flower data set](https://archive.ics.uci.edu/ml/datasets/iris). Later in the course, we'll look at more complicated (and interesting) data sets. We choose the Iris dataset because it's simple, well-understood, and offers a clear demonstration of classification techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/HVL-ML/DAT158/main/notebooks/assets/iris.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(\"https://raw.githubusercontent.com/HVL-ML/DAT158/main/notebooks/assets/iris.png\", width=\"60%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The task is to predict the Iris flower type, either *Iris-Setosa*, *Iris-Versicolor*, or *Iris-Virginica*, from its sepal and petal lengths and widths (\"begerblad\" and \"kronblad\" in Norwegian). \n",
    "\n",
    "> Vocabulary: assigning each data point to a class is called **classification**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Thought starter: Why might we want a machine to classify flowers for us? Can you think of real-world applications where classification might be crucial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, we have to set up our machine learning framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Set up our ML framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Throughout the course, we'll use several libraries over and over. In particular, <a href=\"https://www.datacamp.com/community/tutorials/python-numpy-tutorial\">NumPy</a> for numerical calculations, <a href=\"https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python\">pandas</a> for working with tabular data (and more!), and <a href=\"https://www.datacamp.com/community/tutorials/matplotlib-tutorial-python\">matplotlib</a> to create visualizations. *Familiarize yourself with all three!* The above links are to some short tutorials. You'll find additional resources through the course Canvas page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> In programming, a *library* is like a collection of pre-made tools that help us accomplish specific tasks without having to build everything from scratch. By using libraries, we can save time and utilize the power of community-contributed code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Visualizing your data is a critical step in machine learning. It can give you insights into the nature of your data and how different features are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To make the notebook reproducible, we set the random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# We set up Matplotlib to display plots directly in the notebook:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Your turn:** Read about these and other *magic* commands by running `%magic` in a code cell. You can uncomment the command below and run the cell using `Shift+Enter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#%magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll use `scikit-learn`, a powerful and widely-used Python library for machine learning. It provides simple and efficient tools for data analysis and modeling. Whether you're a novice or an expert, `scikit-learn` offers tools that make implementing algorithms and processing data a breeze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next, we load, explore, and prepare the Iris data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load, explore and prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section, we'll go through the process of:\n",
    "1. Loading the dataset.\n",
    "2. Exploring its features and labels.\n",
    "3. Understanding its structure.\n",
    "4. Converting it to a Pandas dataframe for ease of manipulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Collecting and preparing data for data analysis tasks forms a large part of real-world projects. We'll discuss this in some detail later in the course. In this case, it's easy. Iris is a standard benchmark data set and comes built in to scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Useful tip:** By placing a question mark in front of a Python object you'll get the documentation. (Alternatively, hit `Shift+Tab` in the cell). By using two question marks, you'll get the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#?load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#??load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The key `DESCR` (short for \"DESCRIPTION\") gives you a text description of the Iris dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#iris_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(iris_dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "List available *features* in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "features = iris_dataset['feature_names']\n",
    "print(f\"Features: {features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "List available *labels*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Labels: {iris_dataset['target_names']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How much data do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iris_dataset['data'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are 150 rows and 4 columns of data. Each row is called a *sample*, each column a *feature*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The _labels_ of the data points are stored under the key `target`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iris_dataset['target'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "...coded as 0, 1, and 2, which correspond to the plants listed in `target_names`. Note: Don't confuse `feature_names` (which lists our input variables like sepal length) with `target_names` (which lists our output labels like Iris-Setosa).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iris_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iris_dataset['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "0 corresponds to Iris-Setosa, 1 to Iris-Versicolor and 2 to Iris-Virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It's often very convenient to use Pandas when working with text-based data sets. In the following cell, we collect all the data in a `Pandas dataframe` (a tabular format):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> DataFrames are a fundamental data structure in Pandas, offering a 2-dimensional, size-mutable, and heterogeneous tabular structure with labeled axes (rows and columns). In simpler terms, it's a way to store data in a table format, similar to an Excel sheet, which makes data manipulation and analysis more intuitive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iris_df = pd.DataFrame(iris_dataset['data'], columns=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here are the first ten rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iris_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It's also convenient to add the labels to each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iris_df['label'] = iris_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Challenge: Use `matplotlib` to plot some features against others and color-code by label. Can you visually identify any patterns that might make some species distinguishable from others?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Your turn!: Load another dataset from `sklearn.datasets` and inspect its structure. Can you find the feature names and targets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Great job! You've successfully loaded the Iris dataset, explored its structure, and transformed it into a more manipulatable format using Pandas. With this understanding, we're now in a good position to dive deeper into machine learning tasks. In the upcoming sections, we'll visualize the data, preprocess the data and then train our first machine learning model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Visualizing your data is not just about making pretty plots. It helps in understanding the relationships between data attributes and provides insights into the distribution and nature of the data, which can significantly impact our modeling decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It's interesting to investigate the _distribution_ of the various features. We can use the plotting functionality of Pandas to create **histograms**. A histogram represents the distribution of a single variable by dividing it into bins and showing the frequency of data points in each bin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set the feature name\n",
    "feat = 'sepal length (cm)'\n",
    "\n",
    "# Create a histogram of the specified feature for all the plants\n",
    "fig, ax = plt.subplots()\n",
    "iris_df[feat].hist(ax=ax)\n",
    "ax.set_xlabel(feat)\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(f\"Histogram of {feat} for all plants\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iris_dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set the plant label\n",
    "plant_label = 0 # Iris-Setosa\n",
    "\n",
    "# Create a histogram of the sepal lengths for the specified plant\n",
    "fig, ax = plt.subplots()\n",
    "iris_df[feat][iris_df['label'] == plant_label].hist(ax=ax)\n",
    "ax.set_xlabel('Sepal length (cm)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(f\"Histogram of sepal lengths for {iris_dataset.target_names[plant_label]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Scatter plots** are used to visualize the relationship between two variables. Each dot in the plot represents a single data point, with its position determined by the values of its two attributes. Such plots can help us explore how the various features differ among the different plants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax.scatter(x=iris_df[iris_df['label'] == 0]['sepal length (cm)'], y=iris_df[iris_df['label'] == 0]['sepal width (cm)'], \n",
    "           color='red', label='Iris-Setosa', s=50, marker='o')\n",
    "\n",
    "ax.scatter(x=iris_df[iris_df['label'] == 1]['sepal length (cm)'], y=iris_df[iris_df['label'] == 1]['sepal width (cm)'], \n",
    "           color='blue', label='Iris-Versicolor', s=50, marker='s')\n",
    "\n",
    "ax.scatter(x=iris_df[iris_df['label'] == 2]['sepal length (cm)'], y=iris_df[iris_df['label'] == 2]['sepal width (cm)'], \n",
    "           color='green', label='Iris-Virginica', s=50, marker='^')\n",
    "\n",
    "plt.xlabel('Sepal length (cm)')\n",
    "plt.ylabel('Sepal width (cm)')\n",
    "plt.title('Scatter plot of sepal length versus sepal width, colored by plant class')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Pandas can actually create these plots for us for each pair of features in the data frame. We color the dots by their label using `c = iris_df['label']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(iris_df[features], c=iris_df['label'], figsize=(15,15), marker='o', s=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As we can observe, the colors (representing different Iris species) are quite distinct from each other.\n",
    " **To successfully train a machine learning model to distinguish the classes, therefore, seems promising!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another effective way to visualize pairwise relationships in a dataset is to use pair plots. Seaborn provides an efficient tool for this called `pairplot`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(iris_df, hue='label', palette=\"colorblind\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Visualization of data forms a crucial part of applied machine learning. Carefully designed plots can reveal patterns in the data, which we can exploit when constructing predictive machine learning models. \n",
    "\n",
    "> More generally, **exploratory data analysis** using **plots**, **visualizations**, **statistics**, **probability**, and **baseline models** is a core part of all practical machine learning projects. We'll see that again and again throughout the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Your turn!** To get a feeling for the many different ways you can visualize data, have a quick look at the example gallery of the Python library `seaborn`: https://seaborn.pydata.org/examples/index.html. Feel free to play around with the examples. Here's a guide to the plotting features of Pandas, also worth a look: https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html. You'll make use of these throughout the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Challenge:** Choose one plot from the Seaborn gallery that you find interesting. Try to recreate it using the Iris dataset. What insights can you draw from this new visualization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You've now seen how visualizing your data can not only help in understanding it better but also in identifying patterns and relationships that can be critical when building predictive models. With a good grasp of our data's structure and relationships, we're well-prepared to continue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Feature engineering / selecting features to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Feature engineering is the process of selecting, transforming, or creating the most relevant variables (or \"features\") to improve the performance of our machine learning models. It's a craft that's equal parts art and science, requiring domain knowledge, intuition, and a good understanding of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can choose to train a model on one or more of the features. Let's use `sepal length` and `sepal width`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# We select the 0'th and 1'st columns since these correspond to sepal length and width:\n",
    "X = iris_dataset['data'][:, [0, 1]]\n",
    "y = iris_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now have access to two measurements (sepal length and width) for 150 flowers. From these features stored in X, our model should predict the correct label y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> We're only using two of the four features to construct our predictive models for demonstration purposes. While all the features could provide more information, simplifying our dataset will allow us to better illustrate certain modeling concepts. You'll see why when we try using all four features later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here are the first five measurements and their corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As a reminder from our earlier discussions, 0 corresponds to Iris-Setosa, 1 to Iris-Versicolor, and 2 to Iris-Virginica. (Also, remember that Python lists and arrays are indexed starting from 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list(zip(range(len(y)), iris_dataset['target_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note that data preparation in general, and what's called **feature engineering** in particular, is one of the most critical, time-consuming, and challenging parts of applied machine learning (yet often underappreciated). In fact, the actual modeling work in applied machine learning is a very minor part of real-world machine learning systems development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Most of the time (and code) in machine learning projects is spent on tasks such as collecting data, cleaning data, and especially designing effective features. A well-engineered feature can sometimes make the difference between a poorly performing model and a highly accurate one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Your turn!** Consider the other features we didn't use - petal length and petal width. How might including these influence our model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We're building a machine learning model that should be able to predict Iris species for *new measurements*. To *simulate* such measurements, we leave out a set of data when constructing the model and use it to test the model. This set is called a **test set**. \n",
    "\n",
    "If we train the model using all the data, there's a risk it might overfit, essentially \"remembering\" the data rather than learning from patterns. This can lead to poor predictions on *new* data. It will be bad at **generalization**. \n",
    "\n",
    "We use **training data to construct the model** and **test data to evaluate the model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scikit-learn has a convenient `train_test_split` function that we'll use to set aside 25% of our data (including the corresponding labels) to be used as test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f'Training data: {X_train.shape}\\nTest data: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's collect the training data in a dataframe and display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iris_df = pd.DataFrame(X_train, columns=iris_dataset['feature_names'][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iris_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "...same with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iris_df_test = pd.DataFrame(X_test, columns=iris_dataset['feature_names'][0:2])\n",
    "iris_df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are many different machine learning models available for classification tasks.\n",
    "\n",
    "One of the most versatile and powerful models in the machine learning landscape are the tree-based models, particularly **random forests**, which are ensembles of **decision trees**. You can read about random forests here: http://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees, and about decision trees here: http://scikit-learn.org/stable/modules/tree.html. \n",
    "\n",
    "We'll talk much more about these models later in the course. For now, we'll use random forests as \"black boxes\" without further explanation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42, n_estimators=100) # random_state for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Classification models in scikit-learn are Python classes and always come with `fit` and `predict` methods. `fit` is used to adapt the model to the data (i.e., *train* the model), while `predict` is used to produce predictions.\n",
    "\n",
    "Think of `fit` as a way to teach the model about the patterns in your data and `predict` as a way for the model to apply that learned knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you see, there are a bunch of parameters to choose in a `RandomForestClassifier`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll look at some of these later in the course. For now we're just using default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The model has now made predictions on the test data, and we can check how well it performed. \n",
    "\n",
    "A simple and intuitive metric to gauge model performance is *accuracy*. It measures the proportion of correct predictions out of the total predictions made.\n",
    "\n",
    "Throughout the course, we'll learn about many other ways to evaluate machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Your turn!**\n",
    "- You can also use `rf.score` to find the accuracy. Try that.\n",
    "- Try to change the default parameters in `RandomForestClassifier`. Can you get a better accuracy? Hint: try increasing `n_estimators`. We'll learn about what the parameters mean later in the course.\n",
    "- Try to use all four features in the model (petal and sepal lengths and widths). How does the random forest perform?\n",
    "- Experiment with other classifiers in `scikit-learn`: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning. For a start, try the `SGDClassifier`. Remember to import it first with: `from sklearn.linear_model import SGDClassifier`. Observe how performance changes with different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## All the code gathered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here's the code we used to load the data, select features, split into train and test sets, train a model, predict and evaluate. \n",
    "\n",
    "It's a testament to the power and simplicity of Python libraries like `scikit-learn` that complex processes like training a machine learning model can be accomplished in just a few lines of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load the data and split into data and labels\n",
    "iris_dataset = load_iris()\n",
    "X = iris_dataset['data'][:, [0, 1]]\n",
    "y = iris_dataset['target']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Import and train a machine learning model\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Another example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After exploring the Iris dataset, let's delve into another commonly used dataset in the machine learning community: the **Diabetes Data Set**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Load, explore and prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We repeat the above procedure on another benchmark data set: the Diabetes Data Set. You'll find a description of the data set together with a link to the data here:  https://www.kaggle.com/uciml/pima-indians-diabetes-database. The data set is also available here: https://assets.datacamp.com/production/course_1939/datasets/diabetes.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This dataset originates from the National Institute of Diabetes and Digestive and Kidney Diseases. It aims to predict the onset of diabetes based on certain diagnostic measures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As usual, we'll use Pandas to inspect and process tabular data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://assets.datacamp.com/production/course_1939/datasets/diabetes.csv'\n",
    "diabetes = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Upon inspection, the dataset consists of eight feature columns and one target column indicating the presence (1) or absence (0) of diabetes.\n",
    "\n",
    "More information about the data can be found using `describe` and `info`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "diabetes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "diabetes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It's all numbers, and all the samples contain values for all the features (there are 768 non-NaN values). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here's a description of each feature (from <a href=\"https://www.kaggle.com/uciml/pima-indians-diabetes-database\">Kaggle</a>):\n",
    "- Pregnancies: number of times pregnant\n",
    "- Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "- Diastolic: Diastolic blood pressure (mm Hg)\n",
    "- Triceps: Triceps skin fold thickness (mm)\n",
    "- Insulin: 2-hour serum insulin (mu U/ml)\n",
    "- BMI: Body mass index (weight in kg/(height in m)^2)\n",
    "- DPF: Diabetes pedigree function\n",
    "- Age: Age (years)\n",
    "- Diabetes: Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As before we can visualize the connection between the features in a scatter plot, where each sample is colored by whether it belongs to a diabetic persons or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(diabetes, c=diabetes['diabetes'], figsize=(15,15), marker='o', s=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From the visualization, it's evident that distinguishing between diabetic and non-diabetic samples is more challenging compared to distinguishing Iris species.\n",
    "\n",
    "Still, let's follow the same procedure: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Select which features to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "... this time we'll use all the features to construct our model. We let `X` consist of all columns except `diabetes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = diabetes[diabetes.columns[0:-1]]           \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The target variable is in the `diabetes` column. `1` means diabetes, `0` not diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y = diabetes['diabetes']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We split into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "... and train a random forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42, n_estimators=100) \n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we can make predictions on the test set and compute the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With this model we can predict diabetes from the given features with an accuracy of 73%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Machine learning models are highly generic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Key Observation!** The methodology we employed for the diabetes data mirrors our approach with the Iris dataset, underlining the generality and adaptability of machine learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "iris_dataset = load_iris()\n",
    "X = iris_dataset['data'][:, [0, 1]]\n",
    "y = iris_dataset['target']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Import and train a machine learning model\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "diabetes = pd.read_csv('https://assets.datacamp.com/production/course_1939/datasets/diabetes.csv')\n",
    "X = diabetes[diabetes.columns[0:-1]] \n",
    "y = diabetes['diabetes']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Import and train a machine learning model\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**The essence of the process remains unchanged, emphasizing that the foundational steps in training machine learning models are consistent across datasets.** \n",
    "\n",
    "\n",
    "> **Machine learning models are generic: the same model can be used for many different tasks!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Diving Deeper: Understanding the Model's Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Understanding the predictions and workings of machine learning models is vital. Not only does it allow for improvements in model performance, but it also promotes trust in the model's decisions. For our diabetes classifier, which features weigh heavily for our model's prediction of whether someone has diabetes? How does our model interpret the relationship between BMI and diabetes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Random forests can provide a measure called **feature importance** which reveals the significance of each feature in making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To present these importances in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Find index of those with highest importance, sorted from largest to smallest:\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(X.shape[1]): \n",
    "    print(f'{X.columns[indices[f]]}: {np.round(importances[indices[f]],2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Visualizing these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set the figure size and the number of decimal places to round the importances to\n",
    "fig_size = (12, 8)\n",
    "decimals = 2\n",
    "\n",
    "# Create the horizontal bar plot of feature importances\n",
    "f, ax = plt.subplots(figsize=fig_size)\n",
    "plt.barh(X.columns[indices], np.round(importances[indices], decimals))\n",
    "ax.set_xlabel(\"Relative importance\")\n",
    "ax.set_title(\"Feature importances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The glucose level has the largest explanatory value, followed by BMI. It's not hard to imagine that this kind of information is beneficial in practice! \n",
    "\n",
    "Although glucose level and BMI seem to have the most significant influence, it's crucial to note that interpreting feature importance requires caution. The importance in scikit-learn's random forests is based on measuring how efficient each feature is in reducing uncertainty (the so-called *impurity* in random forests). However, relying solely on these can be misleading. A comprehensive examination on this issue can be found [here](https://explained.ai/rf-importance/index.html).\n",
    "\n",
    "A more reliable technique is **permutation importance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Permutation importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The idea behind permutation importance is that if a feature is important for the predictions of a trained model, then a random shuffling of the data in the corresponding column should drastically reduce the model's accuracy. On the other hand, a random shuffling of an unimportant feature (for a given trained model) shouldn't impact the model much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import the permutation_importance function\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Calculate feature importances using permutation importance\n",
    "n_repeats = 30\n",
    "random_state = 0\n",
    "r = permutation_importance(rf, X_test, y_test,\n",
    "                            n_repeats=n_repeats,\n",
    "                            random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here are the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    print(f\"{X_test.columns[i]:<8}\"\n",
    "    f\"{r.importances_mean[i]:.3f}\"\n",
    "    f\" +/- {r.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "...and as a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get the sorted indices of the features according to mean importance\n",
    "r_sorted_idx = r.importances_mean.argsort()\n",
    "\n",
    "# Set the figure size\n",
    "fig_size = (12, 12)\n",
    "\n",
    "# Create a boxplot\n",
    "fig, ax = plt.subplots(figsize=fig_size)\n",
    "ax.boxplot(\n",
    "    r.importances[r_sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=X_test.columns[r_sorted_idx],\n",
    ")\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel('Feature importance')\n",
    "plt.ylabel('Features')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We observe that `glucose` is the most important feature by far. If you shuffle `glucose`, the model accuracy decreases by more than ten percentage points. The four features with negative weights were useless for the predictions: if you shuffle these at random you will sometimes obtain slightly *better* accuracy!\n",
    "\n",
    "A natural question to ask is, \"If the glucose level increases, does the predicted probability for diabetes increase or decrease?\". Or is there a more complicated connection between the two?\n",
    "\n",
    "We can (partly) uncover this by using partial dependence plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Partial Dependence Plots (PDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The idea is to plot predictions while increasing a specified feature. For example, what happens to the predicted probability of diabetes as the glucose level increases? As this is an important feature, we expect it to change quite a bit (in fact, since we're talking about diabetes, we expect the probability to increase). And what about unimportant features like age? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For `glucose`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import the PartialDependenceDisplay function\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Set the figure size\n",
    "fig_size = (10, 6)\n",
    "\n",
    "# Create a plot of partial dependence for the 'glucose' feature\n",
    "f, ax = plt.subplots(figsize=fig_size)\n",
    "PartialDependenceDisplay.from_estimator(rf, X_train, ['glucose'], kind='both', subsample=50,\n",
    "                                       ax=ax)\n",
    "\n",
    "# Add a title to the plot\n",
    "plt.title('Partial dependence plot for the \"glucose\" feature')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We see that as the glucose level increases the predicted probability for diabetes increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What about `BMI`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "PartialDependenceDisplay.from_estimator(rf, X_train, ['bmi'], kind='both', subsample=50,\n",
    "                                       ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Same for BMI. The plot for age indicates why this feature is deemed unimportant: the probability increases a bit at the beginning (low ages), but then it flattens out. \n",
    "\n",
    "What about `age`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "PartialDependenceDisplay.from_estimator(rf, X_train, ['age'], kind='both',  subsample=50, \n",
    "                                       ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Two-dimensional PDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Typically it's the interaction among several features that influences model predictions, not each feature independently. By plotting two-dimensional interaction plots, one can get a better understanding of how the features affect the predictions.\n",
    "\n",
    "> **Warning:** This takes a while to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12,8))\n",
    "PartialDependenceDisplay.from_estimator(rf, X_train, [('glucose', 'bmi')], kind='average',  subsample=50, \n",
    "                                        ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll learn about other methods to investigate and explain machine learning model predictions later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **IMPORTANT:** It's worth noting that these insights are contingent on the dataset we've used, the model, and the approach to measure feature importance. Meaningful conclusions about the actual relationships between diabetes, glucose, BMI, age, etc., would require rigorous datasets and investigations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Your turn!**\n",
    "- Get the feature importances for the Iris predictions. Is it sepal length or width that provides the most information?\n",
    "- Construct another random forest by changing the `random_state`. What happens to the accuracy? What happens to the feature importances?\n",
    "- Based on the feature importances and permutation importances you've calculated, which features would you consider removing or keeping for a subsequent model iteration?\n",
    "- Using the partial dependence plots, describe the relationship between glucose levels and the likelihood of having diabetes. Is this consistent with your prior expectations?\n",
    "- Try to create a better random forest by tuning the parameters. If you want to dive deep, use `grid_search` (http://scikit-learn.org/stable/modules/grid_search.html) to automatically find good parameters. \n",
    "- Try another classification model. \n",
    "- **Extra challenge**: Here's another well-prepared data set: https://assets.datacamp.com/production/course_1939/datasets/auto.csv. Download it and repeat the procedure above. This time, you will predict a car's country of origin using various features.\n",
    "- **Kaggle competition**: Try out what you've learned here (and elsewhere) in our \"Getting started\" Kaggle competition. See the course Canvas for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Now that we've covered some fundamentals, let's have a first look at how we can turn machine learning models into machine learning powered applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Creating a simple web app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To visualize and interact with our diabetes classifier, we'll create a basic web application. For this, we'll use the `Gradio` library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Gradio is a Python library that allows you to quickly create customizable UI components around your machine learning models, making it easy to show them off to others. We'll study this in much more detail later in the course. In the meantime, you can read more about Gradio here: https://www.gradio.app/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create a prediction function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We've previously trained our diabetes prediction model, which we've named `rf`. Now, let's construct a function that takes the input features and returns the model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def predict_diabetes(age, bmi, glucose):\n",
    "    \"\"\"\n",
    "    Predicts whether a person is diabetic or non-diabetic based on their age, BMI, and glucose level.\n",
    "    \n",
    "    Args:\n",
    "        age (float): The person's age in years.\n",
    "        bmi (float): The person's body mass index (BMI).\n",
    "        glucose (float): The person's glucose level in mg/dL.\n",
    "        \n",
    "    Returns:\n",
    "        str: \"Diabetic\" if the person is predicted to be diabetic, \"Non-diabetic\" otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the mean values for the unused features\n",
    "    mean_insulin = diabetes.insulin.mean()\n",
    "    mean_dpf = diabetes.dpf.mean()\n",
    "    mean_pregnancies = diabetes.pregnancies.mean()\n",
    "    mean_triceps = diabetes.triceps.mean()\n",
    "    mean_diastolic = diabetes.diastolic.mean()\n",
    "\n",
    "    # Reshape the input data for prediction\n",
    "    input_data = np.array([mean_pregnancies, glucose, mean_diastolic, mean_triceps, \n",
    "                           mean_insulin, bmi, mean_dpf, age]).reshape(1, -1)\n",
    "    \n",
    "    # Get the prediction\n",
    "    prediction = rf.predict(input_data)\n",
    "    \n",
    "    # Return the result\n",
    "    return \"Diabetic\" if prediction[0] == 1 else \"Non-diabetic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: For the sake of this demonstration, we're using just three features (`age`, `bmi`, and `glucose`). Typically, you'd want to use all the relevant features your model was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Setting up a Gradio interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With our prediction function in place, we'll set up the Gradio interface for our model. We can customize our input fieldsusing sliders for numerical values such as age, bmi, and glucose, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if (colab or kaggle):\n",
    "    %pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set the minimum, maximum, and default values for the sliders\n",
    "age_min, age_max, age_default = 0, 100, 30\n",
    "bmi_min, bmi_max, bmi_default = 15, 50, 25\n",
    "glucose_min, glucose_max, glucose_default = 0, 200, 100\n",
    "\n",
    "# Create the interface\n",
    "iface = gr.Interface(\n",
    "    fn=predict_diabetes, \n",
    "    inputs=[\n",
    "        gr.components.Slider(minimum=age_min, maximum=age_max, value=age_default, label=\"Age\"),\n",
    "        gr.components.Slider(minimum=bmi_min, maximum=bmi_max, value=bmi_default, label=\"BMI\"),\n",
    "        gr.components.Slider(minimum=glucose_min, maximum=glucose_max, value=glucose_default, label=\"Glucose Level\")\n",
    "    ], \n",
    "    outputs=gr.components.Textbox(label=\"Prediction\"),\n",
    "    title=\"Diabetes Predictor\",\n",
    "    description=\"\"\"Enter your age, BMI, and glucose level to predict whether you are diabetic or non-diabetic.\n",
    "    Data source: Pima Indians Diabetes Database; Model: Random Forest Classifier\"\"\",\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Executing this code initializes and launches a web interface. You'll be presented with sliders corresponding to Age, BMI, and Glucose Level. Once you adjust these values, hitting the \"Submit\" button will display whether the model predicts the input set to be indicative of diabetes.\n",
    "\n",
    "To conclude your testing, simply close the browser tab. If you're executing the code in a terminal, pressing `Ctrl+C` will also terminate the interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## The significance of deploying prototype applications in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As budding machine learning engineers with a software engineering foundation, you're already acquainted with the value of prototyping in traditional software development. In machine learning, prototyping isn't just beneficialit's indispensable.\n",
    "\n",
    "When transitioning from software engineering to machine learning engineering, the dynamics change. Rather than focusing solely on building features or improving functionality, you're dealing with training data, model accuracy, feature importance, and more. Here, rapid iteration becomes even more necessary. Deploying simple prototype applications, even rudimentary ones, facilitates this iterative process.\n",
    "\n",
    "As you'll learn later in the course, the machine learning engineering pipeline revolves around a continuous loop of data ingestion, data preparation, modeling, and deployment:\n",
    "\n",
    "<center>\n",
    "<img width=60% src=\"https://raw.githubusercontent.com/HVL-ML/DAT158/main/notebooks/assets/MLlifecycle.png\">\n",
    "</center>\n",
    "\n",
    "We'll study this in more detail later. For now, here are som reasons to deploy early prototypes:\n",
    "\n",
    "1. **You validate assumptions**: Before investing extensive time refining your model, you can quickly gauge if your model's predictions align with real-world expectations.\n",
    "\n",
    "2. **You identify data issues**: By interacting with a live model, you might spot inconsistencies or areas where your model is lacking, leading you back to the data preparation phase.\n",
    "\n",
    "3. **You encourage collaboration**: Stakeholders, domain experts, and other team members can interact with your model, offering valuable feedback that might not have been apparent from just looking at accuracy metrics.\n",
    "\n",
    "4. **You ease the transition to production**: A working prototype can streamline the process of integrating the final model into a production environment.\n",
    "\n",
    "In essence, deploying prototype applications in machine learning mirrors the agile methodology from software development: build, test, get feedback, and iterate. As we delve deeper into the complexities of the machine learning engineering pipeline, you'll recognize that this iterative philosophy isn't just a best practiceit's foundational to producing effective, robust, and real-world-ready machine learning solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT158",
   "language": "python",
   "name": "dat158"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
