{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A.S. Lundervold, v041022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=60% src=\"https://github.com/alu042/DAT158-2022/raw/main/notebooks/assets/random_forest_detailed.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are **ensembles** of decision trees. As we've seen, a single decision tree is typically susceptible to small changes in the training data, and they tend to overfit. However, we can mitigate these problems by constructing multiple decision trees in a particular way and then combining them to make predictions. As well as being less sensitive and less prone to overfitting, the resulting models, called **random forests**, have much higher performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembling techniques are related to the concept of the <a href=\"https://en.wikipedia.org/wiki/Wisdom_of_the_crowd\">wisdom of the crowd</a>. In many situations, a crowd of non-expert that combines their predictions outperform individual experts.\n",
    "\n",
    "The idea is not new, of course. Here's Aristotle pointing this out back in 350 B.C.E:\n",
    "> *For the many, of whom each individual is but an ordinary person, when they meet together may very likely be better than the few good, if regarded not individually but collectively, just as a feast to which many contribute is better than a dinner provided out of a single purse* <br>&ndash; Aristotle, <a href=\"http://classics.mit.edu/Aristotle/politics.3.three.html\">Politics Book III</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ensembling* is a general technique for combining machine learning models to make even more powerful models. \n",
    "\n",
    "There are several variants of this: for example **voting**, **bagging**, **boosting**, **stacking**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very powerful and widely used model made using ensembling are **random forests**, based on what's called **bagging**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory\n",
    "# or on Kaggle, as that makes some difference for the code below.\n",
    "# We'll do this in every notebook of the course.\n",
    "try:\n",
    "    import colab\n",
    "    colab=True\n",
    "except:\n",
    "    colab=False\n",
    "\n",
    "import os\n",
    "kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display plots directly in the notebook:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the notebook reproducible\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory in which to store data\n",
    "NB_DIR = Path.cwd()       # Set NB_DIR to be the current working directory\n",
    "DATA = NB_DIR/'data'      # The data dir is the subdirectory 'data' under NB_DIR\n",
    "\n",
    "DATA.mkdir(exist_ok=True) # Create the data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen, single decision trees tend to be very sensitive to the exact training data it's fed and also tend to overfit.\n",
    "By training multiple decision trees on different parts of the training data, each one may overfit and be very sensitive. Still, by averaging their predictions, the sensitivity is reduced and the overfitting combatted.\n",
    "\n",
    "Let's say we want to build 500 trees on a training set containing n_samples points. Each tree is built in the following way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Each tree is trained on different data sets: Choose one data point at random for the training data, **with replacement**. Do this `n_samples` times. That is, until you have `n_samples` points. This is called **bootstrap sampling** or **bagging**.\n",
    "2. Each tree trained on different features: Train a decision tree on this data set, but at each node of the tree, **select a random set of features to consider when splitting**, up to `max_features` of them.  \n",
    "\n",
    "This is a random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions from a random forest are made differently for regression and classification: \n",
    "\n",
    "- **Regression:** Let each tree make a prediction, then average them.\n",
    "- **Classification:** Let each tree make a prediction and use a **soft voting strategy** to combine them. As we saw in the decision trees notebook, each tree provides a probability for its prediction. Average these probabilities and predict the class that has the highest average probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=60% src=\"https://github.com/alu042/DAT158-2022/raw/main/notebooks/assets/random_forest_detailed.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two simple examples of using random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out on the two data sets studied in the notebook on decision trees: the **diabetes data set** (classification) and the **Boston housing data** set (regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification: the diabetes data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data = pd.read_csv('https://assets.datacamp.com/production/course_1939/datasets/diabetes.csv')\n",
    "diabetes_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes_data.drop('diabetes', axis=1)\n",
    "y = diabetes_data['diabetes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests have some hyperparameters, mostly inherited from those for decision trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important ones when using random forests in practice are:\n",
    "* the parameters acting as regularization for the decision trees discussed in the previous notebook: i.e. `max_depth` (arguably the most important one, and often the only regularization set to non-default values), `max_features`, `min_samples_split`, `min_samples_leaf`, `max_leaf_nodes`, `min_impurity_decrease`, `min_weight_fraction_leaf`.\n",
    "* `n_estimators`: the number of decision trees in the forest. Increase to get increased model expressiveness. \n",
    "* `n_jobs`: set to `-1` to use all available CPUs. Useful when training random forests on large data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the trained model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression: the California housing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing(data_home='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data=housing.data, columns=housing.feature_names)\n",
    "y = housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_reg = RandomForestRegressor(random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Voting and model averaging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The voting ideas used by the decision trees of a random forest are applicable more generally.\n",
    "\n",
    "It's not necessary that the models in the ensemble are the same or, if they are, that they're trained on random samples of the training data.\n",
    "\n",
    "One could, e.g., train multiple different models, for example, several polynomial regression models and multiple random forests, and then have them vote on the class or value of instances during training and inference.\n",
    "\n",
    "Scikit-learn's `VotingClassifier` and `VotingRegressor` are helpful for this. We'll take a look at how it can be used at the end of the notebook.\n",
    "\n",
    "Another option simpler that can give increased performance is to train a bunch of slightly different versions of the same model.\n",
    "\n",
    "For example: here's a simple way to combine multiple random forest regressors trained on the above California housing data set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 20\n",
    "\n",
    "# Make an array to store all the models' predictions\n",
    "all_predictions = np.zeros(X_test.shape[0])\n",
    "\n",
    "for i in range(n_models):\n",
    "    rf = RandomForestRegressor(random_state=seed+i, n_jobs=-1) # Note the random state updates\n",
    "    rf.fit(X_train, y_train)\n",
    "    all_predictions+=rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final predictions are the average of the n_models models predictions:\n",
    "y_pred = all_predictions/ n_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slight improvement. :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "> Producing strong learners from an ensemble of weak learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another widely used and powerful class of tree-based machine learning models are those based on **boosted trees**. With this tool added to your toolbelt, you're well-equipped to face most machine learning modeling tasks you'll meet in real life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've learned that random forests are ensembles of decision trees, making predictions by simply averaging all the models in the ensemble. **Boosting** takes another approach to ensembling: new members are added to the ensemble _sequentially_, and each new model is trained on the _errors_ of the ensemble constructed so far, iteratively learning from its mistakes. Again, we'll focus on boosting based on decision trees, as these are the most common base models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main boosting techniques is **AdaBoost**. Here each additional tree focuses on the examples (i.e., _instances_) that were misclassified by the previous trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **gradient boosting**, each tree added to the ensemble tries to predict the *residual error* from the previously added tree. In other words, it's trained to predict the difference between the correct value and the value predicted from the ensemble so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each individual tree is what's called a **weak learner**, unable to fit the data very well. But by combining them by having each tree try to predict the residual of the previous trees' predictions, the result is a very **strong learner**. In fact, we can end up with some of the strongest learners we know. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details about gradient boosting can be found [here](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/) or [here]() (we will base our discussion partly on the one in the first link)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost: Adaptive boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost was introduced in 1995 ([here](https://www.sciencedirect.com/science/article/pii/S002200009791504X)) and was the first boosting algorithm with practical use cases. It won the authors the 2003 Gödel Prize for outstanding journal article in theoretical computer science because of the algorithm's _\"elegance, simplicity of implementation, its wide applicability, and its striking success in reducing errors in benchmark applications even while its theoretical assumptions are not known to hold\"_. \n",
    "\n",
    "AdaBoost _\"set off an explosion of research in the fields of statistics, artificial intelligence, experimental machine learning, and data mining\"_ ([source](http://eatcs.org/index.php/component/content/article/505))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is relatively simple: \n",
    "\n",
    "- Give each instance in the training data an _instance weight_, initially set to 1/m where m is the number of instances.\n",
    "- Extract a sample of the training data and train a *weak learner* on it. The weak learners in AdaBoost are typically decision trees of depth 1 (so-called *decision stumps*), but other models can be used. \n",
    "- Calculate a weight for the learner based on its error rate. Higher accuracy means higher weight.\n",
    "- Then increase the weight of the misclassified instances according to the learner's weight. An instance misclassified by a high-weight learner gets higher weight than one misclassified by a low-weight learner.\n",
    "- Extract another sample from the training data, sampled according to the instance weights. \n",
    "- Train the next weak learner on the sampled data, compute the learner weight and update the weights of the training instances accordingly. \n",
    "- Continue until perfect fit or `n_estimators` have been trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each new learner added to the ensemble corrects the shortcomings of previous models as it focuses on high-weight instances. In other words, the weighing of the training data makes the new learner focus more on difficult cases.\n",
    "\n",
    "Predictions from AdaBoost are made by producing predictions from all the weak learners, weighted according to their weights.\n",
    "\n",
    "Here's an illustration from the textbook by A. Geron:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=50% src=\"https://github.com/alu042/DAT158-2022/raw/main/notebooks/assets/adaboost.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost in more detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the above procedure a bit clearer, here are the steps for classification using AdaBoost in more detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start, we don't know which instances are most difficult, so we give them the same weight, making sure that the sum of all weights is 1: set the weights of all $m$ instances to $w^{(i)} = 1/m$ (note that the sum of all instance weights is 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a weak learner on a sample of the training data\n",
    "2. Get the error rate of the learner on the sample:\n",
    "\n",
    "$$r_i = \\frac{\\mbox{sum of weights of errors}}{\\mbox{sum of errors}} = \\frac{\\underset{\\hat{y}_j^{(i)} \\neq y_j}{\\sum w^{(i)}}}{\\sum w^{(i)}}$$\n",
    "\n",
    "3. Compute the learner's weight:\n",
    "\n",
    "$$\\alpha_j = \\eta \\log \\frac{1-r_j}{r_j},$$ where $\\eta$ is the learning rate. (See below for an explanation of the formula.)\n",
    "\n",
    "4. Update all the weights of the misclassified instances:\n",
    "\n",
    "$$\\mbox{For  } \\hat{y}_j^{(i)} \\neq y^{(i)}: \\quad w^{(i)} := w^{(i)} \\exp(\\alpha_j),$$\n",
    "\n",
    "and normalize them to keep the sum equal to 1: $$w^{(i)} := \\frac{w^{(i)}}{\\sum w^{(i)}}.$$\n",
    "\n",
    "Repeat 1-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **A closer look at the learner weights:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The error rate of a learner is between 0 and 1\n",
    "r = np.arange(0.001, 1, 0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The formula for the learner weights\n",
    "learning_rate = 1\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(r, learning_rate*np.log((1-r)/r))\n",
    "plt.ylim([-5,5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Higher error rate means lower learner weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out on the California Housing data provided by scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try a single decision stump (our weak learner):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_reg = DecisionTreeRegressor(max_depth=1, random_state=seed)\n",
    "dt_reg.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, dt_reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..then an AdaBoost ensemble of 10 decision stumps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_reg = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=1), \n",
    "                             n_estimators=10, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_reg.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, ada_reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significantly better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** this was an unusually simple AdaBoostRegressor, just for demonstration and comparison with decision trees. Here's a more reasonable model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_reg = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=None), \n",
    "                             n_estimators=50, random_state=seed)\n",
    "\n",
    "ada_reg.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, ada_reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting: gradient descent + boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In AdaBoost, the learners added to the ensemble try to correct their predecessors by changing the weights on the data points. Gradient boosting has the same objective, but rather than tweaking the instance weights; each new learner attempts to predict the _residual error_ of the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual errors: the basic idea behind gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try to build such a boosting ensemble ourselves. First, we'll see the idea in a spreadsheet, then in code. Gradient boosting builds on this, as we shall see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual errors and boosting using a spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Go to http://bit.ly/boosting_example for boosting explained in a spreadsheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual errors and boosting using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use some randomly generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need some new data on which to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(-0.5, 0.5, 500)\n",
    "X_new = X_new.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial model will be the mean value of the target (as that is the value minimizing mean squared error):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_0 = DummyRegressor(strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_0.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our initial model and our goal is to improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X[:, 0], y, 'b.')\n",
    "ax.plot(X_new, F_0.predict(X_new), 'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll train a single decision tree on the residuals of the initial model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0 = y - F_0.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "h_0 = DecisionTreeRegressor(max_depth=2)\n",
    "h_0.fit(X,y_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first ensemble is now simply the sum of $F_0$ and $h_0$: $$F_1 = F_0 + h_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look at its predictions on some new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A convenience function for plotting predictions of our ensembles\n",
    "def plot_predictions(models):\n",
    "    y_pred = sum(model.predict(X_new) for model in models)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(X[:, 0], y, 'b.')\n",
    "    ax.plot(X_new, y_pred, 'g-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions([F_0, h_0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a larger ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals from previous ensemble\n",
    "F_1_pred = F_0.predict(X) + h_0.predict(X) \n",
    "y1 = y - F_1_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitted to the residuals:\n",
    "h_1 = DecisionTreeRegressor(max_depth=2)\n",
    "h_1.fit(X,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated ensemble:\n",
    "F_2_pred = F_1_pred + h_1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals:\n",
    "y2 = y - F_2_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitted to the residuals:\n",
    "h_2 = DecisionTreeRegressor(max_depth=2)\n",
    "h_2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated ensemble:\n",
    "F_3_pred = F_2_pred + h_2.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the initial ensemble results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions([F_0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions([F_0, h_0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding a second model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions([F_0, h_0, h_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding another model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions([F_0, h_0, h_1, h_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Magic!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue this for as long as we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting(k=3, X=X, y=y):\n",
    "    \"\"\"\n",
    "    Trains k trees on the randomly generated points\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct models\n",
    "    models = []\n",
    "    \n",
    "    ## Initial model:\n",
    "    F_0 = DummyRegressor(strategy='mean')\n",
    "    F_0.fit(X,y)\n",
    "    y = y - F_0.predict(X)\n",
    "    models.append(F_0)\n",
    "    \n",
    "    ## Creating the ensemble\n",
    "    for i in range(k):\n",
    "        h = DecisionTreeRegressor(max_depth=2)\n",
    "        h.fit(X,y)\n",
    "        models.append(h)\n",
    "        y = y - h.predict(X)\n",
    "    \n",
    "    plot_predictions(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an interactive plot showcasing the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interactive, IntSlider, fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot = interactive(boosting, X=fixed(X), y=fixed(y),\n",
    "                              k = IntSlider(min=0, max=50, step=1, value=0))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual errors and the boosting algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above recursive procedure can be described as follows:\n",
    "\n",
    "* Create an initial model, $F_0$\n",
    "* Fit a model to the residuals, $h_0(x) \\sim y - F_0(x)$\n",
    "* Create an ensemble, $F_1(x) = F_0(x) + h_0(x)$\n",
    "* Fit a model to the residuals, $h_1(x) \\sim y - F_1(x)$\n",
    "* Create an ensemble, $F_2(x) = F_1(x) + h_1(x)$\n",
    "* ... continue until reaching a stopping criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where's the gradient in gradient boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that mathematically, the above procedure is actually doing gradient descent using the mean squared error loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error loss function is\n",
    "\n",
    "$$L(y, \\hat{y}) = \\frac 1m \\sum_{i=1}^m (y_i - \\hat{y}_i)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its partial derivatives with respect to the $\\hat{y}_j$ are \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\hat{y}_j} L(y,\\hat{y}) &= \\frac1m \\frac{\\partial}{\\partial \\hat{y}_j} (y_j - \\hat{y}_j) \\\\ \n",
    "&= -\\frac2m(y_j - \\hat{y}_j)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is, therefore\n",
    "\n",
    "$$\\nabla_{\\hat{y}} L(y,\\hat{y}) = -\\frac2m (y - \\hat{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we're doing $F_m(x) = F_{m-1}(x) + h_m(x)$ where $h_m(x)$ is a model fitted to the residuals $y-\\hat{y}$ of the previous model, we're actually doing a gradient descent on the mean squared error loss function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More precisely, when we're adding a new model to the ensemble, we're adding the negative of the gradient of the mean squared error loss function up to a constant (called the learning rate in both gradient descent and in gradient boosting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This view of things provides the fundamental insight behind gradient boosting: we can swap out the mean squared loss function with any other (differentiable) loss function $L(y, \\hat{y})$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](https://www.gormanalysis.com/blog/gradient-boosting-explained) for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#??GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on the housing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=3, n_estimators=20, \n",
    "                                 learning_rate=1.0, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, gbrt.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's vote!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate model averaging through voting, let's ensemble a few models and have them vote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = RandomForestRegressor(random_state=seed, n_jobs=-1)\n",
    "\n",
    "model2 = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=None), \n",
    "                           n_estimators=50, random_state=seed)\n",
    "\n",
    "model3 = GradientBoostingRegressor(max_depth=3, n_estimators=20, learning_rate=1.0, \n",
    "                                  random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?VotingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = VotingRegressor([('rf', model1), \n",
    "                            ('ada', model2),\n",
    "                            ('gb', model3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, ensemble.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Better than all the models used on their own!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [model1, model2, model3]:\n",
    "    m.fit(X_train, y_train)\n",
    "    mse = mean_squared_error(y_test, m.predict(X_test))\n",
    "    print(f'MSE: {mse}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other advanced boosting techniques and libraries worth knowing about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple libraries for gradient boosting that can provide significantly more powerful models than scikit-learn. Two extremely widely used ones (you've undoubtedly come across them on Kaggle) are\n",
    "\n",
    "- XGBoost: A widely used implementation of gradient boosting. Here are some sources for information: https://xgboost.readthedocs.io/en/latest, https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost, https://campus.datacamp.com/courses/extreme-gradient-boosting-with-xgboost/classification-with-xgboost\n",
    "- LightGBM: Another often-used gradient boosting library (in fact it's recently become more popular than XGBoost on Kaggle). Have a look at the documentation here: https://lightgbm.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use **stacking** where multiple model predictions are combined by training a *blender* (instead of simply voting), take a look at scikit-learn's `StackingClassifier` and `StackingRegressor`, or ML-Ensemble https://github.com/flennerhag/mlens and vecstack https://github.com/vecxoz/vecstack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an illustration of the idea from the textbook by A. Geron:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=30% src=\"https://github.com/alu042/DAT158-2022/raw/main/notebooks/assets/stack.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
