{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 05.10.2022, A. S. Lundervold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds on `DAT158-2.1-LinReg_GradientDescent.ipynb` and covers the following topics:\n",
    "\n",
    "- **Polynomial regression:** Our data is often not neatly arranged along a straight line. We'll see how linear regression can be modified to handle data generated in more complex ways. We'll use this as motivation for the following three concepts below (which form the most crucial part of this discussion).\n",
    "\n",
    "\n",
    "- **Learning curves:** This is a handy way to investigate whether your model is overfitting the training data.\n",
    "\n",
    "\n",
    "- **Bias/variance tradeoff:** An essential concept in supervised machine learning, which will help you build better models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also Geron's notebook here: https://github.com/ageron/handson-ml2/blob/master/04_training_linear_models.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook will repeat some of what we covered in Part 1, in the notebook `DAT158-1.2-extra-Intro_to_ML.ipynb`.** It's helpful to study both of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up our standard environment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To automatically reload modules defined in external files.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# To display plots directly in the notebook:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the notebook reproducible\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some data by using the quadratic function $2 + x + \\frac12x^2$ with some added noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m,1) - 3\n",
    "y = 2 + X + 0.5 * X**2 + np.random.randn(m,1)\n",
    "\n",
    "plt.plot(X,y, 'b.')\n",
    "plt.axis([-3,3,0,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to fit a straight line to these points using `scikit-learn` as in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "theta = [lin_reg.intercept_, lin_reg.coef_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...use it to predict on a new value and plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[-3], [3]]) # We choose two new x-values to create our predicted straight line\n",
    "y_predict = lin_reg.predict(X_new)\n",
    "\n",
    "plt.plot(X_new, y_predict, 'r')\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..not very impressive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not surprising, as we're trying to fit a straight line to data generated from something more complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, we can still use `LinearRegression`; we just have to add more features that turn our straight line into a polynomial. That is, we do linear regression in a higher dimension.\n",
    "\n",
    "Instead of using the polynomial\n",
    "\n",
    "$$y = \\theta_0x_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n$$\n",
    "\n",
    "we use\n",
    "\n",
    "$$y = \\theta_0x_0 + \\theta_1 x_1 + \\theta_2 x_1^2 + \\theta_3 x_1^3  + \\cdots + \\theta_k x_1^k + \\theta_{k+1} x_2 + \\theta_{k+2} x_2^2 + \\cdots \\theta_{nk} x_n^k$$\n",
    "\n",
    "This is called **polynomial regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try it out by adding a new feature to `X` that is the square of the values in `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_square = np.dstack((X, X**2))\n",
    "X_square = X_square[:,0,:] # This tweak is needed to make LinearRegression happy below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first few values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_square[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the right column consists of the squares of the values in the left column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use linear regression on these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_square = LinearRegression()\n",
    "lin_reg_square.fit(X_square, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and then predict values for a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "# Add squares:\n",
    "X_new_square = np.dstack((X_new, X_new**2))\n",
    "X_new_square = X_new_square[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = lin_reg_square.predict(X_new_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Great!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have more parameters than for plain linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_square.intercept_, lin_reg_square.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "\"%.2f\" % lin_reg_square.coef_[0][0]": "0.93",
     "\"%.2f\" % lin_reg_square.coef_[0][1]": "0.56",
     "\"%.2f\" % lin_reg_square.intercept_[0]": "1.78"
    }
   },
   "source": [
    "Our model produced the function \n",
    "\n",
    "{{\"%.2f\" % lin_reg_square.intercept_[0]}} + {{\"%.2f\" % lin_reg_square.coef_[0][0]}} x + {{\"%.2f\" % lin_reg_square.coef_[0][1]}} x^2\n",
    "\n",
    "while the true function was \n",
    "\n",
    "$2 + x + \\frac12 x^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the hard way to do it.. Of course, scikit-learn has built-in tools for creating polynomial features which lead to the exact same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly[:5] # Original feature of X and the square of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher order polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not restricted to using only degree 2 polynomials. It's straightforward to add polynomial features of any degree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try degree 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, it's important to scale the data when using high-dimensional models (and when using gradient descent, as discussed earlier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to perform these three steps:\n",
    "\n",
    "<img src=\"https://github.com/alu042/DAT158-2022/raw/main/notebooks/assets/pipeline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're in a situation where you're taking more than a couple of steps to get from your data to predictions, you should use scikit-learn's `Pipeline` module, as you did in Assignment 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following corresponds to the above diagram:\n",
    "polynomial_regression = make_pipeline(\n",
    "                                    PolynomialFeatures(degree=degree, include_bias=False),\n",
    "                                    StandardScaler(),\n",
    "                                    LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit using the entire pipeline:\n",
    "polynomial_regression.fit(X, y)\n",
    "\n",
    "# Predict on new data (using the pipeline):\n",
    "y_new = polynomial_regression.predict(X_new)\n",
    "\n",
    "# Plot the result\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make functions that investigates the effect of adding degrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(degree):\n",
    "    poly_reg = make_pipeline(PolynomialFeatures(degree=degree, \n",
    "                                                include_bias=False),\n",
    "                                                StandardScaler(),\n",
    "                                                LinearRegression())\n",
    "    \n",
    "    return poly_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_poly_regression(degree, ax=None):\n",
    "    \n",
    "    polynomial_regression = get_pipeline(degree)\n",
    "    polynomial_regression.fit(X, y)\n",
    "    y_new = polynomial_regression.predict(X_new)\n",
    "    if not ax: fig, ax = plt.subplots()\n",
    "    ax.axis([-3, 3, 0, 10])\n",
    "    ax.plot(X, y, \"b.\")\n",
    "    ax.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "    ax.set_title(f'Degree {degree}')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_poly_regression(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_poly_regression(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_poly_regression(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_poly_regression(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_poly_regression(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1, 2, 20, 300]\n",
    "fig, axes = plt.subplots(2,2, figsize=(14,10))\n",
    "for i,ax in enumerate(axes.flat):     \n",
    "    plot_poly_regression(degrees[i], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that as we increase the degree, we get a better and better fit to our data. However, as we get to higher and higher degrees, our model is less and less reasonable. It fits the training data well, but it tries too hard and ends up very different from what we know is the true data generating process (a polynomial of degree 2).\n",
    "\n",
    "This is, as we know, what's called **overfitting**. Our model fails to **generalize** as it gets more and more complex.\n",
    "\n",
    "> Even if a model fits well with the training data, it's not guaranteed to perform well on unseen test data, even if it gets 100% accuracy on the training data! This is the problem of **generalization**: what you're really interested in is models whose performance **generalize to new data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, we've used train-test-splits and cross-validation to help us detect when models were overfitting the training data. Now, let's look at another very useful tool for investigating model performance and doing model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is to simultaneously measure the model's performance on the training data and on a validation data set as we increase the total data set size. It's the same concept as in human learning, where learning typically increases with experience:\n",
    "\n",
    "<img width=60% src=\"https://github.com/alu042/DAT158-2022/raw/main/notebooks/assets/wikipedia_learning_curves.png\"><br><small><center>Image from <a href=\"https://commons.wikimedia.org/wiki/File:Alanf777_Lcd_fig01.png\">Wikipedia</a></center></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve this we\n",
    "1. Split the data into training and validation using `train_test_split`\n",
    "2. Calculate the mean squared error for the model on the training and validation data as we increase the data size from 1 to all data points.\n",
    "3. Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function taken from the textbook by A. GÃ©ron\n",
    "def plot_learning_curves(model, X, y):\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "    \n",
    "    # For storing the results\n",
    "    train_errors, val_errors = [], []\n",
    "    \n",
    "    # Calculate RMSE as we increase the data size\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "\n",
    "    # Plot the results\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    plt.legend(loc=\"upper right\", fontsize=14) \n",
    "    plt.xlabel(\"Training set size\", fontsize=14)\n",
    "    plt.ylabel(\"RMSE\", fontsize=14)\n",
    "    plt.title('Learning curves')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out on our data using polynomial regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves_linreg(degree, axis=[0, 80, 0, 3]):\n",
    "    poly_reg = get_pipeline(degree)\n",
    "    _ = plot_poly_regression(degree)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plot_learning_curves(poly_reg, X, y)\n",
    "    plt.axis(axis)\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves_linreg(degree=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves_linreg(degree=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves_linreg(degree=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two critical points to notice based on these plots:**\n",
    "\n",
    "1. In the learning curve for standard linear regression (degree=1), we notice that the curves quickly stabilize to a relatively high error rate. Feeding the models more data doesn't help. This is what an **underfitting** model's learning curves typically look like: high training error and validation error close to training error. As we shall see, another way to say the same thing is that the model has a **high bias**.\n",
    "\n",
    "\n",
    "2. When we use a high degree polynomial, for example, degree=10, we get a learning curve where there's a gap between the training and validation performance. The model scores very well on the training set but fails to obtain a similar score on the validation set. This is typical for models that are **overfitting** the training data and failing to generalize to new data: very low training error and much higher validation error. In other words, the model has **high variance**, as explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Learning curves can be used to investigate the choice of K in a K-fold cross validation: a given choice of K can lead to different amounts of bias. One should try to avoid choosing K in such a way that the training sets in each fold become too small to provide good estimates of generalization performance (i.e., overestimates of the error). Using learning curves, one can get a sense of the number of training instances needed in each fold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: scikit-learn has a built-in method for creating learning curves. Have a look at <a href=\"http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\">Plotting Learning Curves</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The bias/variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a crucial concept in supervised machine learning. Good knowledge of this concept helps you become a better machine learning engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias and variance are two different sources of errors for machine learning models:\n",
    "\n",
    "\n",
    "* The **bias** of a model comes from the simplifying assumptions made by the model to make the target function easier to approximate. In a high bias model, the assumptions don't fit well with the data (for example, assuming that the data is linear when it's really quadratic), typically resulting in underfitting.\n",
    "\n",
    "\n",
    "* The **variance** of a model is an estimate of how much the model changes based on changes in the training data. In other words, how sensitive the model is to the precise data set. The high-degree polynomial above was very sensitive to the training data set because of its many degrees of freedom. A high variance model will tend to overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/alu042/DAT158-2022/raw/main/notebooks/assets/biasvariance.png\"><br><small><center>Image from http://scott.fortmann-roe.com/docs/BiasVariance.html</center></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a third source of errors for machine learning models: **irreducible error**. This comes from inherent uncertainty or noise in the data, which we can't fix unless we change the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversimplified models tend to have high bias, underfitting both the training and test data.  Increasing their complexity will lower their bias but increase their variance, making them poor at generalization.\n",
    "\n",
    "> In general: the simpler the model, the higher the bias. The more complex the model, the higher the variance.\n",
    "\n",
    "What you want is to find the sweet spot where the total error caused by the bias and the variance (and the irreducible error) is lowest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/alu042/DAT158-2022/raw/main/notebooks/assets/bias_variance_tradeoff.png\"><br><small><center>Image from http://scott.fortmann-roe.com/docs/BiasVariance.html</center></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In other words, you want to trade bias off with variance to minimize the generalization error (estimated using the test set). As we know, one cannot use the test set performance for model selection. Hence one has to use validation data to determine model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Mathematically:** Assume there is a relationship of the form $y = f(X) + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,\\sigma_{\\epsilon})$ represents the error due to noise. \n",
    "\n",
    "Using linear regression, or any other model, we estimate a model $\\hat{f}(X)$ of $f(X)$. The expected squared predicted error at point x is $$\\mbox{Err}(x) = E\\big[\\big(y - \\hat{f}(x)\\big)^2\\big].$$\n",
    "\n",
    "This can be decomposed into bias and variance components: $$\\mbox{Err}(x) = \\big(E[\\hat{f}(x)] - f(x)\\big)^2 + E\\big[ \\big(\\hat{f}(x) - E[\\hat{f}(x)]\\big)^2 \\big]+ \\sigma_{\\epsilon}^2$$\n",
    "\n",
    "In other words:\n",
    "$$\\mbox{Err}(x) = Bias^2 + Variance + \\mbox{Irreducible error}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our polynomial regression experiments above, the degree 2 and 3 seemed to give the best models. We can verify this by plotting the mean squared error for increasing degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_reg = get_pipeline(degree=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = range(1,17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores, validation_scores = validation_curve(poly_reg, X, y,\n",
    "                                                   param_name='polynomialfeatures__degree',\n",
    "                                                   param_range=degrees,\n",
    "                                                   scoring='neg_mean_squared_error', \n",
    "                                                   cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(degrees, -validation_scores.mean(axis=1), lw=2,\n",
    "         label='validation')\n",
    "plt.plot(degrees, -train_scores.mean(axis=1), lw=2, label='training')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How do we lower the variance of complex models? How do we force models to not overfit without losing generalization performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularization** is the answer, and we'll cover it in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!**\n",
    "- Find a good value for degree by using `GridSearchCV` on our `polynomial_regression` estimator.\n",
    "\n",
    "> **Hint:** the degree inside the `polynomialfeatures` part of the `polynomial_regression` can be accessed via `polynomialfeatures__degree`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
